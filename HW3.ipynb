{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install scikit-learn\n",
    "# %pip install tensorflow\n",
    "# %pip install scikeras\n",
    "# %pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 05m 21s]\n",
      "val_AUC: 0.7912253141403198\n",
      "\n",
      "Best val_AUC So Far: 0.8005800843238831\n",
      "Total elapsed time: 04h 19m 08s\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set Up Environment\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress warnings\n",
    "os.environ[\"TF_INTER_OP_PARALLELISM_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_INTRA_OP_PARALLELISM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "#  Set seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Load Data ---\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "train[\"label\"] = train[\"label\"].astype(int)\n",
    "X = train.drop(columns=[\"label\"])\n",
    "y = train[\"label\"]\n",
    "\n",
    "#  Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "#  Split Data: 90% Train, 10% Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.1, stratify=y, random_state=42)\n",
    "\n",
    "# --- 1) Define the Model with Hyperparameter Tuning ---\n",
    "def build_higgs_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(28,)))  # 28 features\n",
    "\n",
    "    # Tune number of hidden layers (between 2 and 8)\n",
    "    num_layers = hp.Int(\"num_layers\", 2, 8)\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        # Tune number of neurons per layer (100-500)\n",
    "        units = hp.Int(\"units\", min_value=100, max_value=500, step=50)\n",
    "        model.add(Dense(units, kernel_regularizer=tf.keras.regularizers.l2(hp.Choice(\"l2\", [1e-5, 1e-4, 5e-4, 1e-3]))))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        # Tune activation function (LeakyReLU or Swish)\n",
    "        activation = hp.Choice(\"activation\", [\"leaky_relu\", \"swish\", \"tanh\"])\n",
    "        if activation == \"leaky_relu\":\n",
    "            model.add(LeakyReLU(alpha=0.1))\n",
    "        elif activation == \"swish\":\n",
    "            model.add(tf.keras.layers.Activation(\"swish\"))\n",
    "        else:  # activation == \"tanh\"\n",
    "            model.add(tf.keras.layers.Activation(\"tanh\"))\n",
    "\n",
    "        # Tune dropout rate (0.2 - 0.6)\n",
    "        model.add(Dropout(hp.Float(\"dropout\", 0.2, 0.6, step=0.05)))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "    # Tune optimizer (Adam, RMSprop, SGD)\n",
    "    optimizer_name = hp.Choice(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\"])\n",
    "    learning_rate = hp.Choice(\"learning_rate\", [0.0005, 0.001, 0.002, 0.003, 0.005])\n",
    "    \n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "# --- 2) Setup Hyperparameter Tuning with Bayesian Optimization ---\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_higgs_model,\n",
    "    objective=\"val_AUC\",\n",
    "    max_trials=50,  # Increase search space\n",
    "    executions_per_trial=1,\n",
    "    directory=\"higgs_tuning_1\",\n",
    "    project_name=\"higgs_optimization\",\n",
    ")\n",
    "\n",
    "#  Early Stopping & LR Reduction\n",
    "early_stopping = EarlyStopping(monitor='val_AUC', patience=10, restore_best_weights=True, mode='max')\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_AUC', factor=0.5, patience=5, verbose=1, mode='max')\n",
    "\n",
    "#  Run the Tuning Process\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,  # Reduce epochs per run\n",
    "    batch_size=128,  #  Default batch size (will be tuned later)\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'num_layers': 5, 'units': 100, 'l2': 1e-05, 'activation': 'swish', 'dropout': 0.2, 'optimizer': 'rmsprop', 'learning_rate': 0.002}\n",
      "Best Batch Size: 128\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.6041 - loss: 0.6954 - val_AUC: 0.7322 - val_loss: 0.6083 - learning_rate: 0.0020\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7109 - loss: 0.6255 - val_AUC: 0.7531 - val_loss: 0.5895 - learning_rate: 0.0020\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7373 - loss: 0.6063 - val_AUC: 0.7609 - val_loss: 0.5825 - learning_rate: 0.0020\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7487 - loss: 0.5963 - val_AUC: 0.7680 - val_loss: 0.5764 - learning_rate: 0.0020\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7568 - loss: 0.5895 - val_AUC: 0.7710 - val_loss: 0.5742 - learning_rate: 0.0020\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7610 - loss: 0.5865 - val_AUC: 0.7749 - val_loss: 0.5704 - learning_rate: 0.0020\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7655 - loss: 0.5821 - val_AUC: 0.7772 - val_loss: 0.5673 - learning_rate: 0.0020\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7730 - loss: 0.5752 - val_AUC: 0.7788 - val_loss: 0.5660 - learning_rate: 0.0020\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7732 - loss: 0.5753 - val_AUC: 0.7791 - val_loss: 0.5666 - learning_rate: 0.0020\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7768 - loss: 0.5712 - val_AUC: 0.7826 - val_loss: 0.5633 - learning_rate: 0.0020\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7803 - loss: 0.5680 - val_AUC: 0.7838 - val_loss: 0.5611 - learning_rate: 0.0020\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7836 - loss: 0.5644 - val_AUC: 0.7857 - val_loss: 0.5595 - learning_rate: 0.0020\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7862 - loss: 0.5610 - val_AUC: 0.7867 - val_loss: 0.5588 - learning_rate: 0.0020\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7833 - loss: 0.5652 - val_AUC: 0.7897 - val_loss: 0.5561 - learning_rate: 0.0020\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7903 - loss: 0.5582 - val_AUC: 0.7905 - val_loss: 0.5552 - learning_rate: 0.0020\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7910 - loss: 0.5573 - val_AUC: 0.7896 - val_loss: 0.5552 - learning_rate: 0.0020\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7914 - loss: 0.5566 - val_AUC: 0.7914 - val_loss: 0.5535 - learning_rate: 0.0020\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7922 - loss: 0.5561 - val_AUC: 0.7920 - val_loss: 0.5543 - learning_rate: 0.0020\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7947 - loss: 0.5537 - val_AUC: 0.7920 - val_loss: 0.5540 - learning_rate: 0.0020\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7955 - loss: 0.5534 - val_AUC: 0.7914 - val_loss: 0.5543 - learning_rate: 0.0020\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7993 - loss: 0.5484 - val_AUC: 0.7937 - val_loss: 0.5525 - learning_rate: 0.0020\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7971 - loss: 0.5505 - val_AUC: 0.7941 - val_loss: 0.5522 - learning_rate: 0.0020\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7979 - loss: 0.5510 - val_AUC: 0.7923 - val_loss: 0.5543 - learning_rate: 0.0020\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7995 - loss: 0.5490 - val_AUC: 0.7950 - val_loss: 0.5515 - learning_rate: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7993 - loss: 0.5489 - val_AUC: 0.7933 - val_loss: 0.5535 - learning_rate: 0.0020\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8010 - loss: 0.5475 - val_AUC: 0.7957 - val_loss: 0.5506 - learning_rate: 0.0020\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8010 - loss: 0.5470 - val_AUC: 0.7960 - val_loss: 0.5507 - learning_rate: 0.0020\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8002 - loss: 0.5479 - val_AUC: 0.7953 - val_loss: 0.5512 - learning_rate: 0.0020\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8027 - loss: 0.5461 - val_AUC: 0.7967 - val_loss: 0.5504 - learning_rate: 0.0020\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8029 - loss: 0.5457 - val_AUC: 0.7981 - val_loss: 0.5494 - learning_rate: 0.0020\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8066 - loss: 0.5414 - val_AUC: 0.7977 - val_loss: 0.5499 - learning_rate: 0.0020\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8059 - loss: 0.5429 - val_AUC: 0.7962 - val_loss: 0.5510 - learning_rate: 0.0020\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8062 - loss: 0.5428 - val_AUC: 0.7965 - val_loss: 0.5521 - learning_rate: 0.0020\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8088 - loss: 0.5395 - val_AUC: 0.7991 - val_loss: 0.5494 - learning_rate: 0.0020\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8066 - loss: 0.5424 - val_AUC: 0.7994 - val_loss: 0.5485 - learning_rate: 0.0020\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8077 - loss: 0.5406 - val_AUC: 0.7999 - val_loss: 0.5495 - learning_rate: 0.0020\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8080 - loss: 0.5415 - val_AUC: 0.7990 - val_loss: 0.5503 - learning_rate: 0.0020\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8089 - loss: 0.5400 - val_AUC: 0.7976 - val_loss: 0.5511 - learning_rate: 0.0020\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8090 - loss: 0.5400 - val_AUC: 0.7968 - val_loss: 0.5516 - learning_rate: 0.0020\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8104 - loss: 0.5378 - val_AUC: 0.7995 - val_loss: 0.5502 - learning_rate: 0.0020\n",
      "Epoch 41/200\n",
      "\u001b[1m342/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.8118 - loss: 0.5372\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8118 - loss: 0.5372 - val_AUC: 0.7985 - val_loss: 0.5534 - learning_rate: 0.0020\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8160 - loss: 0.5316 - val_AUC: 0.7999 - val_loss: 0.5490 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8158 - loss: 0.5323 - val_AUC: 0.8001 - val_loss: 0.5495 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8194 - loss: 0.5270 - val_AUC: 0.7997 - val_loss: 0.5502 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8172 - loss: 0.5299 - val_AUC: 0.7999 - val_loss: 0.5500 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8190 - loss: 0.5288 - val_AUC: 0.7994 - val_loss: 0.5517 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8218 - loss: 0.5245 - val_AUC: 0.7998 - val_loss: 0.5507 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m349/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.8224 - loss: 0.5249\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8224 - loss: 0.5249 - val_AUC: 0.7991 - val_loss: 0.5512 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8225 - loss: 0.5242 - val_AUC: 0.8002 - val_loss: 0.5508 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8241 - loss: 0.5221 - val_AUC: 0.8004 - val_loss: 0.5503 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8243 - loss: 0.5213 - val_AUC: 0.8015 - val_loss: 0.5504 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8255 - loss: 0.5198 - val_AUC: 0.8019 - val_loss: 0.5477 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8256 - loss: 0.5200 - val_AUC: 0.8010 - val_loss: 0.5490 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8250 - loss: 0.5208 - val_AUC: 0.8011 - val_loss: 0.5498 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8263 - loss: 0.5195 - val_AUC: 0.8021 - val_loss: 0.5476 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8272 - loss: 0.5174 - val_AUC: 0.8016 - val_loss: 0.5498 - learning_rate: 5.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8290 - loss: 0.5163 - val_AUC: 0.8018 - val_loss: 0.5493 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8284 - loss: 0.5161 - val_AUC: 0.8011 - val_loss: 0.5510 - learning_rate: 5.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8265 - loss: 0.5186 - val_AUC: 0.8009 - val_loss: 0.5507 - learning_rate: 5.0000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m343/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.8288 - loss: 0.5155\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8288 - loss: 0.5155 - val_AUC: 0.8001 - val_loss: 0.5503 - learning_rate: 5.0000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8281 - loss: 0.5162 - val_AUC: 0.8012 - val_loss: 0.5504 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8296 - loss: 0.5150 - val_AUC: 0.8015 - val_loss: 0.5503 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8280 - loss: 0.5168 - val_AUC: 0.8009 - val_loss: 0.5511 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8308 - loss: 0.5136 - val_AUC: 0.8006 - val_loss: 0.5514 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m342/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.8320 - loss: 0.5118\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8320 - loss: 0.5119 - val_AUC: 0.8012 - val_loss: 0.5502 - learning_rate: 2.5000e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Retrieve Best Hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_batch_size = best_hps.values.get(\"batch_size\", 128)  #  Get best batch size (default to 128 if missing)\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_hps.values}\")\n",
    "print(f\"Best Batch Size: {best_batch_size}\")\n",
    "\n",
    "#  Train the Best Model on Full Data (Using the Tuned Batch Size)\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,  # Train longer\n",
    "    batch_size=best_batch_size,  #  Now using the best batch size\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "#  Plot Training vs Validation AUC\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['AUC'], label='Training AUC', color='blue')\n",
    "plt.plot(history.history['val_AUC'], label='Validation AUC', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Training vs Validation AUC')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 1/20 | HP Config 1 | Seed 0\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.6162 - loss: 0.6862 - val_AUC: 0.7119 - val_loss: 0.6256 - learning_rate: 0.0020\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7090 - loss: 0.6263 - val_AUC: 0.7367 - val_loss: 0.6037 - learning_rate: 0.0020\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7370 - loss: 0.6060 - val_AUC: 0.7527 - val_loss: 0.5898 - learning_rate: 0.0020\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7460 - loss: 0.5981 - val_AUC: 0.7593 - val_loss: 0.5836 - learning_rate: 0.0020\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7550 - loss: 0.5910 - val_AUC: 0.7684 - val_loss: 0.5759 - learning_rate: 0.0020\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7612 - loss: 0.5845 - val_AUC: 0.7708 - val_loss: 0.5741 - learning_rate: 0.0020\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7663 - loss: 0.5803 - val_AUC: 0.7748 - val_loss: 0.5700 - learning_rate: 0.0020\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7695 - loss: 0.5772 - val_AUC: 0.7761 - val_loss: 0.5689 - learning_rate: 0.0020\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7725 - loss: 0.5741 - val_AUC: 0.7801 - val_loss: 0.5655 - learning_rate: 0.0020\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7766 - loss: 0.5706 - val_AUC: 0.7831 - val_loss: 0.5625 - learning_rate: 0.0020\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7792 - loss: 0.5686 - val_AUC: 0.7823 - val_loss: 0.5636 - learning_rate: 0.0020\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7815 - loss: 0.5659 - val_AUC: 0.7847 - val_loss: 0.5615 - learning_rate: 0.0020\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7822 - loss: 0.5651 - val_AUC: 0.7870 - val_loss: 0.5584 - learning_rate: 0.0020\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7867 - loss: 0.5611 - val_AUC: 0.7884 - val_loss: 0.5577 - learning_rate: 0.0020\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7846 - loss: 0.5638 - val_AUC: 0.7897 - val_loss: 0.5569 - learning_rate: 0.0020\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7872 - loss: 0.5608 - val_AUC: 0.7898 - val_loss: 0.5564 - learning_rate: 0.0020\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7904 - loss: 0.5571 - val_AUC: 0.7891 - val_loss: 0.5584 - learning_rate: 0.0020\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7926 - loss: 0.5555 - val_AUC: 0.7890 - val_loss: 0.5578 - learning_rate: 0.0020\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7935 - loss: 0.5538 - val_AUC: 0.7906 - val_loss: 0.5551 - learning_rate: 0.0020\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7939 - loss: 0.5533 - val_AUC: 0.7899 - val_loss: 0.5572 - learning_rate: 0.0020\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7939 - loss: 0.5540 - val_AUC: 0.7910 - val_loss: 0.5547 - learning_rate: 0.0020\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7975 - loss: 0.5507 - val_AUC: 0.7920 - val_loss: 0.5543 - learning_rate: 0.0020\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7984 - loss: 0.5492 - val_AUC: 0.7947 - val_loss: 0.5541 - learning_rate: 0.0020\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7981 - loss: 0.5504 - val_AUC: 0.7924 - val_loss: 0.5562 - learning_rate: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7998 - loss: 0.5478 - val_AUC: 0.7918 - val_loss: 0.5561 - learning_rate: 0.0020\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8027 - loss: 0.5447 - val_AUC: 0.7896 - val_loss: 0.5580 - learning_rate: 0.0020\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7996 - loss: 0.5486 - val_AUC: 0.7923 - val_loss: 0.5563 - learning_rate: 0.0020\n",
      "Epoch 28/200\n",
      "\u001b[1m338/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8032 - loss: 0.5441\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8032 - loss: 0.5441 - val_AUC: 0.7924 - val_loss: 0.5569 - learning_rate: 0.0020\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8054 - loss: 0.5425 - val_AUC: 0.7957 - val_loss: 0.5525 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8081 - loss: 0.5385 - val_AUC: 0.7965 - val_loss: 0.5522 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8095 - loss: 0.5372 - val_AUC: 0.7950 - val_loss: 0.5534 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8101 - loss: 0.5367 - val_AUC: 0.7969 - val_loss: 0.5530 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8081 - loss: 0.5387 - val_AUC: 0.7958 - val_loss: 0.5532 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8126 - loss: 0.5340 - val_AUC: 0.7959 - val_loss: 0.5526 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8117 - loss: 0.5346 - val_AUC: 0.7948 - val_loss: 0.5552 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8160 - loss: 0.5290 - val_AUC: 0.7952 - val_loss: 0.5547 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m336/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8118 - loss: 0.5353\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8119 - loss: 0.5352 - val_AUC: 0.7955 - val_loss: 0.5529 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8156 - loss: 0.5295 - val_AUC: 0.7985 - val_loss: 0.5511 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8170 - loss: 0.5282 - val_AUC: 0.7979 - val_loss: 0.5519 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8182 - loss: 0.5272 - val_AUC: 0.7983 - val_loss: 0.5507 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8190 - loss: 0.5257 - val_AUC: 0.7973 - val_loss: 0.5520 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8174 - loss: 0.5280 - val_AUC: 0.7973 - val_loss: 0.5531 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m342/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8195 - loss: 0.5252\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8195 - loss: 0.5252 - val_AUC: 0.7972 - val_loss: 0.5534 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8199 - loss: 0.5245 - val_AUC: 0.7972 - val_loss: 0.5517 - learning_rate: 2.5000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8219 - loss: 0.5230 - val_AUC: 0.7980 - val_loss: 0.5507 - learning_rate: 2.5000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8189 - loss: 0.5253 - val_AUC: 0.7977 - val_loss: 0.5513 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8218 - loss: 0.5220 - val_AUC: 0.7976 - val_loss: 0.5520 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m336/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8201 - loss: 0.5238\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8202 - loss: 0.5238 - val_AUC: 0.7975 - val_loss: 0.5519 - learning_rate: 2.5000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 2/20 | HP Config 1 | Seed 100\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.6043 - loss: 0.6975 - val_AUC: 0.7208 - val_loss: 0.6268 - learning_rate: 0.0020\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7083 - loss: 0.6276 - val_AUC: 0.7466 - val_loss: 0.5975 - learning_rate: 0.0020\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7339 - loss: 0.6086 - val_AUC: 0.7581 - val_loss: 0.5858 - learning_rate: 0.0020\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7463 - loss: 0.5980 - val_AUC: 0.7651 - val_loss: 0.5787 - learning_rate: 0.0020\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7552 - loss: 0.5909 - val_AUC: 0.7716 - val_loss: 0.5730 - learning_rate: 0.0020\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7608 - loss: 0.5857 - val_AUC: 0.7764 - val_loss: 0.5691 - learning_rate: 0.0020\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7674 - loss: 0.5795 - val_AUC: 0.7756 - val_loss: 0.5685 - learning_rate: 0.0020\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7701 - loss: 0.5778 - val_AUC: 0.7793 - val_loss: 0.5652 - learning_rate: 0.0020\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7759 - loss: 0.5713 - val_AUC: 0.7828 - val_loss: 0.5616 - learning_rate: 0.0020\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7769 - loss: 0.5709 - val_AUC: 0.7835 - val_loss: 0.5607 - learning_rate: 0.0020\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7794 - loss: 0.5683 - val_AUC: 0.7838 - val_loss: 0.5604 - learning_rate: 0.0020\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7817 - loss: 0.5663 - val_AUC: 0.7850 - val_loss: 0.5588 - learning_rate: 0.0020\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7861 - loss: 0.5618 - val_AUC: 0.7887 - val_loss: 0.5554 - learning_rate: 0.0020\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7869 - loss: 0.5619 - val_AUC: 0.7879 - val_loss: 0.5578 - learning_rate: 0.0020\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7861 - loss: 0.5622 - val_AUC: 0.7904 - val_loss: 0.5548 - learning_rate: 0.0020\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7896 - loss: 0.5591 - val_AUC: 0.7915 - val_loss: 0.5533 - learning_rate: 0.0020\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7888 - loss: 0.5602 - val_AUC: 0.7917 - val_loss: 0.5534 - learning_rate: 0.0020\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7903 - loss: 0.5583 - val_AUC: 0.7918 - val_loss: 0.5537 - learning_rate: 0.0020\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7933 - loss: 0.5549 - val_AUC: 0.7924 - val_loss: 0.5537 - learning_rate: 0.0020\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7948 - loss: 0.5536 - val_AUC: 0.7920 - val_loss: 0.5527 - learning_rate: 0.0020\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7954 - loss: 0.5525 - val_AUC: 0.7948 - val_loss: 0.5506 - learning_rate: 0.0020\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7981 - loss: 0.5508 - val_AUC: 0.7929 - val_loss: 0.5531 - learning_rate: 0.0020\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7967 - loss: 0.5521 - val_AUC: 0.7944 - val_loss: 0.5517 - learning_rate: 0.0020\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7983 - loss: 0.5512 - val_AUC: 0.7940 - val_loss: 0.5526 - learning_rate: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7993 - loss: 0.5497 - val_AUC: 0.7945 - val_loss: 0.5513 - learning_rate: 0.0020\n",
      "Epoch 26/200\n",
      "\u001b[1m342/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8016 - loss: 0.5462\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8016 - loss: 0.5462 - val_AUC: 0.7947 - val_loss: 0.5546 - learning_rate: 0.0020\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8040 - loss: 0.5439 - val_AUC: 0.7992 - val_loss: 0.5476 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8075 - loss: 0.5403 - val_AUC: 0.7989 - val_loss: 0.5492 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8075 - loss: 0.5398 - val_AUC: 0.7988 - val_loss: 0.5491 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8078 - loss: 0.5401 - val_AUC: 0.7991 - val_loss: 0.5478 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8076 - loss: 0.5397 - val_AUC: 0.7982 - val_loss: 0.5489 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m339/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8105 - loss: 0.5368\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8105 - loss: 0.5367 - val_AUC: 0.7990 - val_loss: 0.5483 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8133 - loss: 0.5341 - val_AUC: 0.7994 - val_loss: 0.5482 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8170 - loss: 0.5289 - val_AUC: 0.8006 - val_loss: 0.5476 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8162 - loss: 0.5299 - val_AUC: 0.7997 - val_loss: 0.5479 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8151 - loss: 0.5305 - val_AUC: 0.8010 - val_loss: 0.5460 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8159 - loss: 0.5305 - val_AUC: 0.8011 - val_loss: 0.5474 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8172 - loss: 0.5297 - val_AUC: 0.8011 - val_loss: 0.5468 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8191 - loss: 0.5257 - val_AUC: 0.8011 - val_loss: 0.5474 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8168 - loss: 0.5284 - val_AUC: 0.8011 - val_loss: 0.5471 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8180 - loss: 0.5270 - val_AUC: 0.8011 - val_loss: 0.5480 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8191 - loss: 0.5271 - val_AUC: 0.8009 - val_loss: 0.5475 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8200 - loss: 0.5248 - val_AUC: 0.8012 - val_loss: 0.5468 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8205 - loss: 0.5247 - val_AUC: 0.8004 - val_loss: 0.5475 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m344/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8179 - loss: 0.5280\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8179 - loss: 0.5279 - val_AUC: 0.8003 - val_loss: 0.5477 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8202 - loss: 0.5244 - val_AUC: 0.8004 - val_loss: 0.5480 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8184 - loss: 0.5267 - val_AUC: 0.8008 - val_loss: 0.5474 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8217 - loss: 0.5225 - val_AUC: 0.8012 - val_loss: 0.5476 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8227 - loss: 0.5218 - val_AUC: 0.8015 - val_loss: 0.5479 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8231 - loss: 0.5207 - val_AUC: 0.8016 - val_loss: 0.5469 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8228 - loss: 0.5214 - val_AUC: 0.8016 - val_loss: 0.5467 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8206 - loss: 0.5245 - val_AUC: 0.8015 - val_loss: 0.5473 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8240 - loss: 0.5201 - val_AUC: 0.8012 - val_loss: 0.5478 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8220 - loss: 0.5226 - val_AUC: 0.8014 - val_loss: 0.5478 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m341/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8238 - loss: 0.5194\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8238 - loss: 0.5194 - val_AUC: 0.8014 - val_loss: 0.5476 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8215 - loss: 0.5219 - val_AUC: 0.8016 - val_loss: 0.5475 - learning_rate: 1.2500e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8215 - loss: 0.5228 - val_AUC: 0.8017 - val_loss: 0.5468 - learning_rate: 1.2500e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8226 - loss: 0.5217 - val_AUC: 0.8014 - val_loss: 0.5481 - learning_rate: 1.2500e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8245 - loss: 0.5193 - val_AUC: 0.8012 - val_loss: 0.5480 - learning_rate: 1.2500e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m335/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8241 - loss: 0.5195\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8241 - loss: 0.5195 - val_AUC: 0.8010 - val_loss: 0.5475 - learning_rate: 1.2500e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8233 - loss: 0.5214 - val_AUC: 0.8010 - val_loss: 0.5484 - learning_rate: 6.2500e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8245 - loss: 0.5192 - val_AUC: 0.8011 - val_loss: 0.5476 - learning_rate: 6.2500e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8237 - loss: 0.5198 - val_AUC: 0.8011 - val_loss: 0.5477 - learning_rate: 6.2500e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8267 - loss: 0.5171 - val_AUC: 0.8011 - val_loss: 0.5480 - learning_rate: 6.2500e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8243 - loss: 0.5198\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8243 - loss: 0.5197 - val_AUC: 0.8011 - val_loss: 0.5478 - learning_rate: 6.2500e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8228 - loss: 0.5205 - val_AUC: 0.8012 - val_loss: 0.5477 - learning_rate: 3.1250e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8257 - loss: 0.5178 - val_AUC: 0.8011 - val_loss: 0.5476 - learning_rate: 3.1250e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 3/20 | HP Config 1 | Seed 200\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.6106 - loss: 0.6902 - val_AUC: 0.7297 - val_loss: 0.6165 - learning_rate: 0.0020\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7101 - loss: 0.6270 - val_AUC: 0.7507 - val_loss: 0.5917 - learning_rate: 0.0020\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7344 - loss: 0.6089 - val_AUC: 0.7570 - val_loss: 0.5857 - learning_rate: 0.0020\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7477 - loss: 0.5969 - val_AUC: 0.7676 - val_loss: 0.5771 - learning_rate: 0.0020\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7546 - loss: 0.5909 - val_AUC: 0.7713 - val_loss: 0.5742 - learning_rate: 0.0020\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7607 - loss: 0.5861 - val_AUC: 0.7745 - val_loss: 0.5718 - learning_rate: 0.0020\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7657 - loss: 0.5816 - val_AUC: 0.7807 - val_loss: 0.5666 - learning_rate: 0.0020\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7710 - loss: 0.5767 - val_AUC: 0.7815 - val_loss: 0.5638 - learning_rate: 0.0020\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7741 - loss: 0.5736 - val_AUC: 0.7814 - val_loss: 0.5661 - learning_rate: 0.0020\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7743 - loss: 0.5741 - val_AUC: 0.7861 - val_loss: 0.5603 - learning_rate: 0.0020\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7799 - loss: 0.5679 - val_AUC: 0.7862 - val_loss: 0.5611 - learning_rate: 0.0020\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7814 - loss: 0.5661 - val_AUC: 0.7869 - val_loss: 0.5606 - learning_rate: 0.0020\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7810 - loss: 0.5667 - val_AUC: 0.7870 - val_loss: 0.5597 - learning_rate: 0.0020\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7844 - loss: 0.5637 - val_AUC: 0.7891 - val_loss: 0.5569 - learning_rate: 0.0020\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7863 - loss: 0.5622 - val_AUC: 0.7894 - val_loss: 0.5581 - learning_rate: 0.0020\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7868 - loss: 0.5616 - val_AUC: 0.7907 - val_loss: 0.5553 - learning_rate: 0.0020\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7860 - loss: 0.5626 - val_AUC: 0.7913 - val_loss: 0.5553 - learning_rate: 0.0020\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7911 - loss: 0.5572 - val_AUC: 0.7927 - val_loss: 0.5537 - learning_rate: 0.0020\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7916 - loss: 0.5566 - val_AUC: 0.7950 - val_loss: 0.5511 - learning_rate: 0.0020\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7927 - loss: 0.5547 - val_AUC: 0.7941 - val_loss: 0.5528 - learning_rate: 0.0020\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7942 - loss: 0.5542 - val_AUC: 0.7960 - val_loss: 0.5511 - learning_rate: 0.0020\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7987 - loss: 0.5490 - val_AUC: 0.7944 - val_loss: 0.5542 - learning_rate: 0.0020\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7947 - loss: 0.5540 - val_AUC: 0.7956 - val_loss: 0.5525 - learning_rate: 0.0020\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7980 - loss: 0.5506 - val_AUC: 0.7954 - val_loss: 0.5538 - learning_rate: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8003 - loss: 0.5478 - val_AUC: 0.7962 - val_loss: 0.5523 - learning_rate: 0.0020\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7988 - loss: 0.5504 - val_AUC: 0.7958 - val_loss: 0.5512 - learning_rate: 0.0020\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8024 - loss: 0.5460 - val_AUC: 0.7976 - val_loss: 0.5515 - learning_rate: 0.0020\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8029 - loss: 0.5450 - val_AUC: 0.7972 - val_loss: 0.5515 - learning_rate: 0.0020\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8018 - loss: 0.5467 - val_AUC: 0.7962 - val_loss: 0.5512 - learning_rate: 0.0020\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8035 - loss: 0.5455 - val_AUC: 0.7975 - val_loss: 0.5499 - learning_rate: 0.0020\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8044 - loss: 0.5446 - val_AUC: 0.7961 - val_loss: 0.5530 - learning_rate: 0.0020\n",
      "Epoch 32/200\n",
      "\u001b[1m336/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8042 - loss: 0.5439\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8044 - loss: 0.5438 - val_AUC: 0.7976 - val_loss: 0.5526 - learning_rate: 0.0020\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8082 - loss: 0.5387 - val_AUC: 0.7992 - val_loss: 0.5493 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8121 - loss: 0.5353 - val_AUC: 0.8012 - val_loss: 0.5473 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8109 - loss: 0.5364 - val_AUC: 0.7996 - val_loss: 0.5490 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8097 - loss: 0.5380 - val_AUC: 0.7991 - val_loss: 0.5498 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8142 - loss: 0.5327 - val_AUC: 0.7998 - val_loss: 0.5501 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8111 - loss: 0.5365 - val_AUC: 0.7999 - val_loss: 0.5484 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8135 - loss: 0.5336\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8135 - loss: 0.5335 - val_AUC: 0.7998 - val_loss: 0.5487 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8165 - loss: 0.5304 - val_AUC: 0.8005 - val_loss: 0.5480 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8190 - loss: 0.5270 - val_AUC: 0.8006 - val_loss: 0.5480 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8163 - loss: 0.5296 - val_AUC: 0.8003 - val_loss: 0.5487 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8182 - loss: 0.5284 - val_AUC: 0.8008 - val_loss: 0.5483 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8196 - loss: 0.5266 - val_AUC: 0.8017 - val_loss: 0.5470 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8208 - loss: 0.5246 - val_AUC: 0.8003 - val_loss: 0.5487 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8213 - loss: 0.5244 - val_AUC: 0.8003 - val_loss: 0.5487 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8210 - loss: 0.5250 - val_AUC: 0.8009 - val_loss: 0.5480 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8199 - loss: 0.5252 - val_AUC: 0.8014 - val_loss: 0.5476 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m341/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8190 - loss: 0.5264\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8191 - loss: 0.5262 - val_AUC: 0.8003 - val_loss: 0.5481 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8200 - loss: 0.5253 - val_AUC: 0.8012 - val_loss: 0.5494 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8216 - loss: 0.5238 - val_AUC: 0.8016 - val_loss: 0.5482 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8225 - loss: 0.5224 - val_AUC: 0.8014 - val_loss: 0.5487 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8245 - loss: 0.5204 - val_AUC: 0.8019 - val_loss: 0.5479 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8222 - loss: 0.5224 - val_AUC: 0.8011 - val_loss: 0.5491 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8241 - loss: 0.5208 - val_AUC: 0.8009 - val_loss: 0.5497 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8244 - loss: 0.5199 - val_AUC: 0.8011 - val_loss: 0.5491 - learning_rate: 2.5000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8237 - loss: 0.5208 - val_AUC: 0.8008 - val_loss: 0.5495 - learning_rate: 2.5000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m334/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8255 - loss: 0.5186\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8257 - loss: 0.5183 - val_AUC: 0.8006 - val_loss: 0.5496 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8219 - loss: 0.5227 - val_AUC: 0.8009 - val_loss: 0.5500 - learning_rate: 1.2500e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8243 - loss: 0.5200 - val_AUC: 0.8011 - val_loss: 0.5494 - learning_rate: 1.2500e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8237 - loss: 0.5203 - val_AUC: 0.8010 - val_loss: 0.5496 - learning_rate: 1.2500e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8210 - loss: 0.5240 - val_AUC: 0.8007 - val_loss: 0.5498 - learning_rate: 1.2500e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m347/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8249 - loss: 0.5194\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8249 - loss: 0.5193 - val_AUC: 0.8013 - val_loss: 0.5491 - learning_rate: 1.2500e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 4/20 | HP Config 1 | Seed 300\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.6169 - loss: 0.6869 - val_AUC: 0.7232 - val_loss: 0.6183 - learning_rate: 0.0020\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7136 - loss: 0.6230 - val_AUC: 0.7448 - val_loss: 0.5991 - learning_rate: 0.0020\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7382 - loss: 0.6052 - val_AUC: 0.7571 - val_loss: 0.5867 - learning_rate: 0.0020\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7537 - loss: 0.5927 - val_AUC: 0.7648 - val_loss: 0.5803 - learning_rate: 0.0020\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7594 - loss: 0.5869 - val_AUC: 0.7695 - val_loss: 0.5753 - learning_rate: 0.0020\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7695 - loss: 0.5784 - val_AUC: 0.7730 - val_loss: 0.5737 - learning_rate: 0.0020\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7710 - loss: 0.5768 - val_AUC: 0.7745 - val_loss: 0.5709 - learning_rate: 0.0020\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7765 - loss: 0.5719 - val_AUC: 0.7782 - val_loss: 0.5684 - learning_rate: 0.0020\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7790 - loss: 0.5696 - val_AUC: 0.7805 - val_loss: 0.5648 - learning_rate: 0.0020\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7812 - loss: 0.5671 - val_AUC: 0.7812 - val_loss: 0.5643 - learning_rate: 0.0020\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7811 - loss: 0.5674 - val_AUC: 0.7825 - val_loss: 0.5645 - learning_rate: 0.0020\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7843 - loss: 0.5641 - val_AUC: 0.7828 - val_loss: 0.5654 - learning_rate: 0.0020\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7860 - loss: 0.5625 - val_AUC: 0.7858 - val_loss: 0.5604 - learning_rate: 0.0020\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7881 - loss: 0.5600 - val_AUC: 0.7858 - val_loss: 0.5604 - learning_rate: 0.0020\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7890 - loss: 0.5590 - val_AUC: 0.7884 - val_loss: 0.5574 - learning_rate: 0.0020\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7909 - loss: 0.5580 - val_AUC: 0.7874 - val_loss: 0.5600 - learning_rate: 0.0020\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7920 - loss: 0.5565 - val_AUC: 0.7875 - val_loss: 0.5584 - learning_rate: 0.0020\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7940 - loss: 0.5539 - val_AUC: 0.7897 - val_loss: 0.5574 - learning_rate: 0.0020\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7948 - loss: 0.5542 - val_AUC: 0.7897 - val_loss: 0.5580 - learning_rate: 0.0020\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7952 - loss: 0.5541 - val_AUC: 0.7908 - val_loss: 0.5558 - learning_rate: 0.0020\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7976 - loss: 0.5509 - val_AUC: 0.7904 - val_loss: 0.5566 - learning_rate: 0.0020\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7969 - loss: 0.5518 - val_AUC: 0.7912 - val_loss: 0.5539 - learning_rate: 0.0020\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8012 - loss: 0.5470 - val_AUC: 0.7908 - val_loss: 0.5576 - learning_rate: 0.0020\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8006 - loss: 0.5479 - val_AUC: 0.7902 - val_loss: 0.5573 - learning_rate: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8015 - loss: 0.5469 - val_AUC: 0.7891 - val_loss: 0.5589 - learning_rate: 0.0020\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8016 - loss: 0.5466 - val_AUC: 0.7916 - val_loss: 0.5564 - learning_rate: 0.0020\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8044 - loss: 0.5442 - val_AUC: 0.7919 - val_loss: 0.5556 - learning_rate: 0.0020\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8030 - loss: 0.5457 - val_AUC: 0.7920 - val_loss: 0.5558 - learning_rate: 0.0020\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8038 - loss: 0.5451 - val_AUC: 0.7939 - val_loss: 0.5531 - learning_rate: 0.0020\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8032 - loss: 0.5452 - val_AUC: 0.7931 - val_loss: 0.5550 - learning_rate: 0.0020\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8057 - loss: 0.5427 - val_AUC: 0.7947 - val_loss: 0.5544 - learning_rate: 0.0020\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8068 - loss: 0.5428 - val_AUC: 0.7934 - val_loss: 0.5549 - learning_rate: 0.0020\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8081 - loss: 0.5407 - val_AUC: 0.7920 - val_loss: 0.5579 - learning_rate: 0.0020\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8070 - loss: 0.5418 - val_AUC: 0.7952 - val_loss: 0.5535 - learning_rate: 0.0020\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8099 - loss: 0.5388 - val_AUC: 0.7946 - val_loss: 0.5542 - learning_rate: 0.0020\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8105 - loss: 0.5381 - val_AUC: 0.7951 - val_loss: 0.5541 - learning_rate: 0.0020\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8118 - loss: 0.5363 - val_AUC: 0.7931 - val_loss: 0.5573 - learning_rate: 0.0020\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8105 - loss: 0.5379 - val_AUC: 0.7945 - val_loss: 0.5558 - learning_rate: 0.0020\n",
      "Epoch 39/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8139 - loss: 0.5349\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8139 - loss: 0.5349 - val_AUC: 0.7936 - val_loss: 0.5554 - learning_rate: 0.0020\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8121 - loss: 0.5365 - val_AUC: 0.7967 - val_loss: 0.5532 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8196 - loss: 0.5270 - val_AUC: 0.7965 - val_loss: 0.5536 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8202 - loss: 0.5262 - val_AUC: 0.7970 - val_loss: 0.5527 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8179 - loss: 0.5291 - val_AUC: 0.7966 - val_loss: 0.5541 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8216 - loss: 0.5247 - val_AUC: 0.7960 - val_loss: 0.5551 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8222 - loss: 0.5236 - val_AUC: 0.7976 - val_loss: 0.5512 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8220 - loss: 0.5240 - val_AUC: 0.7958 - val_loss: 0.5542 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8234 - loss: 0.5230 - val_AUC: 0.7956 - val_loss: 0.5549 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8245 - loss: 0.5211 - val_AUC: 0.7961 - val_loss: 0.5531 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8250 - loss: 0.5205 - val_AUC: 0.7961 - val_loss: 0.5555 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m331/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8235 - loss: 0.5227\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8236 - loss: 0.5226 - val_AUC: 0.7959 - val_loss: 0.5556 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8246 - loss: 0.5205 - val_AUC: 0.7974 - val_loss: 0.5537 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8256 - loss: 0.5192 - val_AUC: 0.7972 - val_loss: 0.5529 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8267 - loss: 0.5183 - val_AUC: 0.7974 - val_loss: 0.5527 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8273 - loss: 0.5166 - val_AUC: 0.7975 - val_loss: 0.5536 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8266 - loss: 0.5189 - val_AUC: 0.7983 - val_loss: 0.5522 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8270 - loss: 0.5185 - val_AUC: 0.7984 - val_loss: 0.5518 - learning_rate: 5.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8293 - loss: 0.5152 - val_AUC: 0.7981 - val_loss: 0.5546 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8289 - loss: 0.5158 - val_AUC: 0.7971 - val_loss: 0.5542 - learning_rate: 5.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8281 - loss: 0.5159 - val_AUC: 0.7975 - val_loss: 0.5553 - learning_rate: 5.0000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m346/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8286 - loss: 0.5153\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8286 - loss: 0.5153 - val_AUC: 0.7981 - val_loss: 0.5531 - learning_rate: 5.0000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8305 - loss: 0.5135 - val_AUC: 0.7983 - val_loss: 0.5530 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8318 - loss: 0.5120 - val_AUC: 0.7981 - val_loss: 0.5543 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8309 - loss: 0.5124 - val_AUC: 0.7980 - val_loss: 0.5530 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8317 - loss: 0.5125 - val_AUC: 0.7978 - val_loss: 0.5551 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m335/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8296 - loss: 0.5140\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8297 - loss: 0.5138 - val_AUC: 0.7980 - val_loss: 0.5544 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8318 - loss: 0.5106 - val_AUC: 0.7979 - val_loss: 0.5548 - learning_rate: 1.2500e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 5/20 | HP Config 2 | Seed 0\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.5744 - loss: 0.7334 - val_AUC: 0.6989 - val_loss: 0.6390 - learning_rate: 5.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6599 - loss: 0.6659 - val_AUC: 0.7218 - val_loss: 0.6220 - learning_rate: 5.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6943 - loss: 0.6432 - val_AUC: 0.7355 - val_loss: 0.6105 - learning_rate: 5.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7122 - loss: 0.6279 - val_AUC: 0.7419 - val_loss: 0.6042 - learning_rate: 5.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7269 - loss: 0.6183 - val_AUC: 0.7491 - val_loss: 0.5977 - learning_rate: 5.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7314 - loss: 0.6139 - val_AUC: 0.7548 - val_loss: 0.5929 - learning_rate: 5.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7431 - loss: 0.6041 - val_AUC: 0.7602 - val_loss: 0.5867 - learning_rate: 5.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7463 - loss: 0.6010 - val_AUC: 0.7636 - val_loss: 0.5842 - learning_rate: 5.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7514 - loss: 0.5965 - val_AUC: 0.7650 - val_loss: 0.5827 - learning_rate: 5.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7567 - loss: 0.5921 - val_AUC: 0.7676 - val_loss: 0.5813 - learning_rate: 5.0000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7606 - loss: 0.5887 - val_AUC: 0.7699 - val_loss: 0.5780 - learning_rate: 5.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7638 - loss: 0.5851 - val_AUC: 0.7718 - val_loss: 0.5767 - learning_rate: 5.0000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7644 - loss: 0.5851 - val_AUC: 0.7738 - val_loss: 0.5747 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7706 - loss: 0.5792 - val_AUC: 0.7758 - val_loss: 0.5722 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7712 - loss: 0.5786 - val_AUC: 0.7772 - val_loss: 0.5709 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7758 - loss: 0.5749 - val_AUC: 0.7780 - val_loss: 0.5701 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7758 - loss: 0.5736 - val_AUC: 0.7803 - val_loss: 0.5673 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7761 - loss: 0.5739 - val_AUC: 0.7814 - val_loss: 0.5677 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7765 - loss: 0.5729 - val_AUC: 0.7826 - val_loss: 0.5661 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7781 - loss: 0.5722 - val_AUC: 0.7833 - val_loss: 0.5651 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7809 - loss: 0.5691 - val_AUC: 0.7845 - val_loss: 0.5640 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7821 - loss: 0.5677 - val_AUC: 0.7840 - val_loss: 0.5651 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7837 - loss: 0.5659 - val_AUC: 0.7862 - val_loss: 0.5619 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7836 - loss: 0.5663 - val_AUC: 0.7863 - val_loss: 0.5622 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7873 - loss: 0.5631 - val_AUC: 0.7864 - val_loss: 0.5620 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7877 - loss: 0.5619 - val_AUC: 0.7874 - val_loss: 0.5607 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7880 - loss: 0.5613 - val_AUC: 0.7877 - val_loss: 0.5606 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7890 - loss: 0.5607 - val_AUC: 0.7889 - val_loss: 0.5597 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7897 - loss: 0.5603 - val_AUC: 0.7892 - val_loss: 0.5590 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7898 - loss: 0.5603 - val_AUC: 0.7892 - val_loss: 0.5586 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7923 - loss: 0.5570 - val_AUC: 0.7901 - val_loss: 0.5575 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7922 - loss: 0.5567 - val_AUC: 0.7922 - val_loss: 0.5562 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7940 - loss: 0.5547 - val_AUC: 0.7921 - val_loss: 0.5561 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7973 - loss: 0.5511 - val_AUC: 0.7916 - val_loss: 0.5572 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7975 - loss: 0.5517 - val_AUC: 0.7907 - val_loss: 0.5580 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7969 - loss: 0.5520 - val_AUC: 0.7930 - val_loss: 0.5556 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7972 - loss: 0.5515 - val_AUC: 0.7930 - val_loss: 0.5563 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7998 - loss: 0.5489 - val_AUC: 0.7944 - val_loss: 0.5538 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7993 - loss: 0.5498 - val_AUC: 0.7940 - val_loss: 0.5548 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8002 - loss: 0.5487 - val_AUC: 0.7931 - val_loss: 0.5563 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7992 - loss: 0.5488 - val_AUC: 0.7947 - val_loss: 0.5549 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8013 - loss: 0.5475 - val_AUC: 0.7942 - val_loss: 0.5547 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8026 - loss: 0.5455 - val_AUC: 0.7943 - val_loss: 0.5553 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8021 - loss: 0.5464 - val_AUC: 0.7960 - val_loss: 0.5523 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8025 - loss: 0.5457 - val_AUC: 0.7952 - val_loss: 0.5530 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8039 - loss: 0.5435 - val_AUC: 0.7955 - val_loss: 0.5533 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8042 - loss: 0.5437 - val_AUC: 0.7948 - val_loss: 0.5542 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8052 - loss: 0.5428 - val_AUC: 0.7959 - val_loss: 0.5530 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8049 - loss: 0.5428 - val_AUC: 0.7964 - val_loss: 0.5520 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8076 - loss: 0.5398 - val_AUC: 0.7965 - val_loss: 0.5537 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8092 - loss: 0.5379 - val_AUC: 0.7958 - val_loss: 0.5534 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8079 - loss: 0.5403 - val_AUC: 0.7979 - val_loss: 0.5506 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8080 - loss: 0.5394 - val_AUC: 0.7959 - val_loss: 0.5553 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8100 - loss: 0.5376 - val_AUC: 0.7956 - val_loss: 0.5542 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8111 - loss: 0.5365 - val_AUC: 0.7958 - val_loss: 0.5563 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8097 - loss: 0.5373 - val_AUC: 0.7965 - val_loss: 0.5540 - learning_rate: 5.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8110 - loss: 0.5360\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8110 - loss: 0.5360 - val_AUC: 0.7967 - val_loss: 0.5525 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8101 - loss: 0.5373 - val_AUC: 0.7983 - val_loss: 0.5508 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8145 - loss: 0.5332 - val_AUC: 0.7982 - val_loss: 0.5504 - learning_rate: 2.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8141 - loss: 0.5325 - val_AUC: 0.7980 - val_loss: 0.5508 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8139 - loss: 0.5327 - val_AUC: 0.7979 - val_loss: 0.5511 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8154 - loss: 0.5307 - val_AUC: 0.7986 - val_loss: 0.5505 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8172 - loss: 0.5283 - val_AUC: 0.7993 - val_loss: 0.5506 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8166 - loss: 0.5294 - val_AUC: 0.7984 - val_loss: 0.5515 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8182 - loss: 0.5271 - val_AUC: 0.7982 - val_loss: 0.5516 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8166 - loss: 0.5297 - val_AUC: 0.7985 - val_loss: 0.5513 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8189 - loss: 0.5265 - val_AUC: 0.7982 - val_loss: 0.5516 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8181 - loss: 0.5272\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8181 - loss: 0.5272 - val_AUC: 0.7989 - val_loss: 0.5502 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8187 - loss: 0.5266 - val_AUC: 0.7985 - val_loss: 0.5514 - learning_rate: 1.2500e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8175 - loss: 0.5275 - val_AUC: 0.7990 - val_loss: 0.5506 - learning_rate: 1.2500e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8224 - loss: 0.5217 - val_AUC: 0.7988 - val_loss: 0.5509 - learning_rate: 1.2500e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8232 - loss: 0.5207 - val_AUC: 0.7987 - val_loss: 0.5504 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m346/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8207 - loss: 0.5244\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8207 - loss: 0.5244 - val_AUC: 0.7989 - val_loss: 0.5501 - learning_rate: 1.2500e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 6/20 | HP Config 2 | Seed 100\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.5681 - loss: 0.7316 - val_AUC: 0.6979 - val_loss: 0.6418 - learning_rate: 5.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6541 - loss: 0.6676 - val_AUC: 0.7198 - val_loss: 0.6217 - learning_rate: 5.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6925 - loss: 0.6426 - val_AUC: 0.7339 - val_loss: 0.6117 - learning_rate: 5.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7104 - loss: 0.6296 - val_AUC: 0.7431 - val_loss: 0.6041 - learning_rate: 5.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7288 - loss: 0.6165 - val_AUC: 0.7510 - val_loss: 0.5963 - learning_rate: 5.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7373 - loss: 0.6096 - val_AUC: 0.7578 - val_loss: 0.5896 - learning_rate: 5.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7434 - loss: 0.6050 - val_AUC: 0.7614 - val_loss: 0.5865 - learning_rate: 5.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7493 - loss: 0.5991 - val_AUC: 0.7654 - val_loss: 0.5826 - learning_rate: 5.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7529 - loss: 0.5956 - val_AUC: 0.7678 - val_loss: 0.5803 - learning_rate: 5.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7559 - loss: 0.5935 - val_AUC: 0.7697 - val_loss: 0.5780 - learning_rate: 5.0000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7598 - loss: 0.5899 - val_AUC: 0.7717 - val_loss: 0.5767 - learning_rate: 5.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7622 - loss: 0.5874 - val_AUC: 0.7734 - val_loss: 0.5750 - learning_rate: 5.0000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7655 - loss: 0.5843 - val_AUC: 0.7764 - val_loss: 0.5713 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7660 - loss: 0.5837 - val_AUC: 0.7768 - val_loss: 0.5721 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7706 - loss: 0.5796 - val_AUC: 0.7783 - val_loss: 0.5696 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7719 - loss: 0.5785 - val_AUC: 0.7790 - val_loss: 0.5688 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7754 - loss: 0.5752 - val_AUC: 0.7810 - val_loss: 0.5665 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7785 - loss: 0.5723 - val_AUC: 0.7815 - val_loss: 0.5662 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7785 - loss: 0.5717 - val_AUC: 0.7829 - val_loss: 0.5646 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7790 - loss: 0.5713 - val_AUC: 0.7834 - val_loss: 0.5645 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7807 - loss: 0.5702 - val_AUC: 0.7853 - val_loss: 0.5623 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7843 - loss: 0.5661 - val_AUC: 0.7860 - val_loss: 0.5620 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7830 - loss: 0.5664 - val_AUC: 0.7855 - val_loss: 0.5622 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7834 - loss: 0.5666 - val_AUC: 0.7867 - val_loss: 0.5604 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7867 - loss: 0.5629 - val_AUC: 0.7877 - val_loss: 0.5601 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7869 - loss: 0.5632 - val_AUC: 0.7867 - val_loss: 0.5612 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7884 - loss: 0.5613 - val_AUC: 0.7877 - val_loss: 0.5602 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7895 - loss: 0.5603 - val_AUC: 0.7894 - val_loss: 0.5576 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7901 - loss: 0.5593 - val_AUC: 0.7897 - val_loss: 0.5578 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7910 - loss: 0.5589 - val_AUC: 0.7911 - val_loss: 0.5562 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7918 - loss: 0.5587 - val_AUC: 0.7905 - val_loss: 0.5567 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7928 - loss: 0.5570 - val_AUC: 0.7924 - val_loss: 0.5549 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7955 - loss: 0.5545 - val_AUC: 0.7908 - val_loss: 0.5561 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7940 - loss: 0.5551 - val_AUC: 0.7927 - val_loss: 0.5547 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7937 - loss: 0.5553 - val_AUC: 0.7938 - val_loss: 0.5527 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7977 - loss: 0.5515 - val_AUC: 0.7938 - val_loss: 0.5535 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7963 - loss: 0.5527 - val_AUC: 0.7943 - val_loss: 0.5531 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7995 - loss: 0.5492 - val_AUC: 0.7952 - val_loss: 0.5525 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7979 - loss: 0.5510 - val_AUC: 0.7951 - val_loss: 0.5523 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7984 - loss: 0.5508 - val_AUC: 0.7961 - val_loss: 0.5514 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8018 - loss: 0.5472 - val_AUC: 0.7976 - val_loss: 0.5498 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8021 - loss: 0.5467 - val_AUC: 0.7965 - val_loss: 0.5516 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8005 - loss: 0.5484 - val_AUC: 0.7966 - val_loss: 0.5510 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8017 - loss: 0.5468 - val_AUC: 0.7968 - val_loss: 0.5505 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8027 - loss: 0.5461 - val_AUC: 0.7984 - val_loss: 0.5493 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8049 - loss: 0.5439 - val_AUC: 0.7965 - val_loss: 0.5512 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8032 - loss: 0.5456 - val_AUC: 0.7970 - val_loss: 0.5509 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8060 - loss: 0.5422 - val_AUC: 0.7974 - val_loss: 0.5505 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8029 - loss: 0.5458 - val_AUC: 0.7981 - val_loss: 0.5493 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8054 - loss: 0.5422 - val_AUC: 0.7999 - val_loss: 0.5474 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8056 - loss: 0.5429 - val_AUC: 0.7991 - val_loss: 0.5482 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8070 - loss: 0.5408 - val_AUC: 0.7986 - val_loss: 0.5488 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8090 - loss: 0.5388 - val_AUC: 0.7997 - val_loss: 0.5477 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8078 - loss: 0.5398 - val_AUC: 0.8013 - val_loss: 0.5462 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8082 - loss: 0.5394 - val_AUC: 0.8008 - val_loss: 0.5474 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8087 - loss: 0.5385 - val_AUC: 0.8000 - val_loss: 0.5481 - learning_rate: 5.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8093 - loss: 0.5382 - val_AUC: 0.8012 - val_loss: 0.5465 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8133 - loss: 0.5343 - val_AUC: 0.8017 - val_loss: 0.5464 - learning_rate: 5.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8128 - loss: 0.5340 - val_AUC: 0.8014 - val_loss: 0.5461 - learning_rate: 5.0000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8133 - loss: 0.5327 - val_AUC: 0.8014 - val_loss: 0.5467 - learning_rate: 5.0000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8128 - loss: 0.5332 - val_AUC: 0.8020 - val_loss: 0.5471 - learning_rate: 5.0000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8118 - loss: 0.5352 - val_AUC: 0.7999 - val_loss: 0.5491 - learning_rate: 5.0000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8132 - loss: 0.5334 - val_AUC: 0.8006 - val_loss: 0.5482 - learning_rate: 5.0000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8153 - loss: 0.5311 - val_AUC: 0.8003 - val_loss: 0.5500 - learning_rate: 5.0000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8140 - loss: 0.5325 - val_AUC: 0.8004 - val_loss: 0.5489 - learning_rate: 5.0000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8139 - loss: 0.5331\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8139 - loss: 0.5331 - val_AUC: 0.8005 - val_loss: 0.5471 - learning_rate: 5.0000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8162 - loss: 0.5309 - val_AUC: 0.8023 - val_loss: 0.5462 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8193 - loss: 0.5254 - val_AUC: 0.8023 - val_loss: 0.5460 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8175 - loss: 0.5284 - val_AUC: 0.8035 - val_loss: 0.5452 - learning_rate: 2.5000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8157 - loss: 0.5303 - val_AUC: 0.8036 - val_loss: 0.5450 - learning_rate: 2.5000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8210 - loss: 0.5238 - val_AUC: 0.8036 - val_loss: 0.5463 - learning_rate: 2.5000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8217 - loss: 0.5236 - val_AUC: 0.8028 - val_loss: 0.5460 - learning_rate: 2.5000e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8222 - loss: 0.5231 - val_AUC: 0.8037 - val_loss: 0.5457 - learning_rate: 2.5000e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8217 - loss: 0.5236 - val_AUC: 0.8017 - val_loss: 0.5477 - learning_rate: 2.5000e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8198 - loss: 0.5253 - val_AUC: 0.8012 - val_loss: 0.5490 - learning_rate: 2.5000e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8214 - loss: 0.5241 - val_AUC: 0.8016 - val_loss: 0.5485 - learning_rate: 2.5000e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8215 - loss: 0.5229 - val_AUC: 0.8029 - val_loss: 0.5470 - learning_rate: 2.5000e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m347/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.8227 - loss: 0.5227\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8227 - loss: 0.5227 - val_AUC: 0.8016 - val_loss: 0.5487 - learning_rate: 2.5000e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8230 - loss: 0.5222 - val_AUC: 0.8029 - val_loss: 0.5465 - learning_rate: 1.2500e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8244 - loss: 0.5204 - val_AUC: 0.8029 - val_loss: 0.5470 - learning_rate: 1.2500e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8247 - loss: 0.5194 - val_AUC: 0.8024 - val_loss: 0.5481 - learning_rate: 1.2500e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8245 - loss: 0.5195 - val_AUC: 0.8023 - val_loss: 0.5482 - learning_rate: 1.2500e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8260 - loss: 0.5186\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8260 - loss: 0.5186 - val_AUC: 0.8025 - val_loss: 0.5474 - learning_rate: 1.2500e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 7/20 | HP Config 2 | Seed 200\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - AUC: 0.5669 - loss: 0.7297 - val_AUC: 0.7010 - val_loss: 0.6358 - learning_rate: 5.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6522 - loss: 0.6702 - val_AUC: 0.7232 - val_loss: 0.6219 - learning_rate: 5.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6873 - loss: 0.6463 - val_AUC: 0.7364 - val_loss: 0.6102 - learning_rate: 5.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7118 - loss: 0.6304 - val_AUC: 0.7443 - val_loss: 0.6035 - learning_rate: 5.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7203 - loss: 0.6236 - val_AUC: 0.7516 - val_loss: 0.5959 - learning_rate: 5.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7303 - loss: 0.6158 - val_AUC: 0.7574 - val_loss: 0.5910 - learning_rate: 5.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7430 - loss: 0.6056 - val_AUC: 0.7621 - val_loss: 0.5858 - learning_rate: 5.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7459 - loss: 0.6027 - val_AUC: 0.7636 - val_loss: 0.5842 - learning_rate: 5.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7507 - loss: 0.5991 - val_AUC: 0.7667 - val_loss: 0.5816 - learning_rate: 5.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7525 - loss: 0.5966 - val_AUC: 0.7692 - val_loss: 0.5783 - learning_rate: 5.0000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7573 - loss: 0.5923 - val_AUC: 0.7715 - val_loss: 0.5767 - learning_rate: 5.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7592 - loss: 0.5910 - val_AUC: 0.7744 - val_loss: 0.5737 - learning_rate: 5.0000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7613 - loss: 0.5881 - val_AUC: 0.7751 - val_loss: 0.5725 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7643 - loss: 0.5844 - val_AUC: 0.7770 - val_loss: 0.5711 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7697 - loss: 0.5808 - val_AUC: 0.7800 - val_loss: 0.5674 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7698 - loss: 0.5806 - val_AUC: 0.7776 - val_loss: 0.5703 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7742 - loss: 0.5759 - val_AUC: 0.7804 - val_loss: 0.5685 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7751 - loss: 0.5757 - val_AUC: 0.7806 - val_loss: 0.5674 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7757 - loss: 0.5736 - val_AUC: 0.7838 - val_loss: 0.5638 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7758 - loss: 0.5736 - val_AUC: 0.7852 - val_loss: 0.5629 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7796 - loss: 0.5707 - val_AUC: 0.7853 - val_loss: 0.5626 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7808 - loss: 0.5697 - val_AUC: 0.7866 - val_loss: 0.5606 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7827 - loss: 0.5683 - val_AUC: 0.7865 - val_loss: 0.5619 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7825 - loss: 0.5678 - val_AUC: 0.7872 - val_loss: 0.5600 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7836 - loss: 0.5659 - val_AUC: 0.7891 - val_loss: 0.5585 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7855 - loss: 0.5647 - val_AUC: 0.7883 - val_loss: 0.5594 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7868 - loss: 0.5643 - val_AUC: 0.7910 - val_loss: 0.5567 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7862 - loss: 0.5638 - val_AUC: 0.7916 - val_loss: 0.5561 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7863 - loss: 0.5633 - val_AUC: 0.7917 - val_loss: 0.5557 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7901 - loss: 0.5593 - val_AUC: 0.7913 - val_loss: 0.5564 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7873 - loss: 0.5620 - val_AUC: 0.7920 - val_loss: 0.5558 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7912 - loss: 0.5583 - val_AUC: 0.7918 - val_loss: 0.5554 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7929 - loss: 0.5569 - val_AUC: 0.7926 - val_loss: 0.5551 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7897 - loss: 0.5601 - val_AUC: 0.7937 - val_loss: 0.5536 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7946 - loss: 0.5541 - val_AUC: 0.7932 - val_loss: 0.5548 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7947 - loss: 0.5542 - val_AUC: 0.7944 - val_loss: 0.5530 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7934 - loss: 0.5561 - val_AUC: 0.7953 - val_loss: 0.5518 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7959 - loss: 0.5533 - val_AUC: 0.7954 - val_loss: 0.5517 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7991 - loss: 0.5499 - val_AUC: 0.7947 - val_loss: 0.5523 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7987 - loss: 0.5499 - val_AUC: 0.7948 - val_loss: 0.5530 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7958 - loss: 0.5533 - val_AUC: 0.7963 - val_loss: 0.5508 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7976 - loss: 0.5515 - val_AUC: 0.7954 - val_loss: 0.5515 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7985 - loss: 0.5504 - val_AUC: 0.7971 - val_loss: 0.5496 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8014 - loss: 0.5474 - val_AUC: 0.7983 - val_loss: 0.5483 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8019 - loss: 0.5469 - val_AUC: 0.7976 - val_loss: 0.5495 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8011 - loss: 0.5472 - val_AUC: 0.7970 - val_loss: 0.5491 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8037 - loss: 0.5448 - val_AUC: 0.7980 - val_loss: 0.5489 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8002 - loss: 0.5482 - val_AUC: 0.7978 - val_loss: 0.5488 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8039 - loss: 0.5443 - val_AUC: 0.7984 - val_loss: 0.5485 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8043 - loss: 0.5439 - val_AUC: 0.7992 - val_loss: 0.5482 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8056 - loss: 0.5418 - val_AUC: 0.8000 - val_loss: 0.5465 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8037 - loss: 0.5443 - val_AUC: 0.7993 - val_loss: 0.5480 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8054 - loss: 0.5420 - val_AUC: 0.7996 - val_loss: 0.5473 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8060 - loss: 0.5416 - val_AUC: 0.7992 - val_loss: 0.5483 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8061 - loss: 0.5418 - val_AUC: 0.8000 - val_loss: 0.5469 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8060 - loss: 0.5417 - val_AUC: 0.8011 - val_loss: 0.5479 - learning_rate: 5.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8079 - loss: 0.5402 - val_AUC: 0.7993 - val_loss: 0.5482 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8085 - loss: 0.5395 - val_AUC: 0.8002 - val_loss: 0.5477 - learning_rate: 5.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8087 - loss: 0.5393 - val_AUC: 0.8001 - val_loss: 0.5474 - learning_rate: 5.0000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8093 - loss: 0.5377 - val_AUC: 0.8001 - val_loss: 0.5472 - learning_rate: 5.0000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m343/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8112 - loss: 0.5358\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8112 - loss: 0.5358 - val_AUC: 0.8010 - val_loss: 0.5462 - learning_rate: 5.0000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8116 - loss: 0.5349 - val_AUC: 0.8024 - val_loss: 0.5448 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8130 - loss: 0.5342 - val_AUC: 0.8019 - val_loss: 0.5453 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8141 - loss: 0.5324 - val_AUC: 0.8014 - val_loss: 0.5469 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8140 - loss: 0.5332 - val_AUC: 0.8016 - val_loss: 0.5464 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8166 - loss: 0.5300 - val_AUC: 0.8012 - val_loss: 0.5466 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m346/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8156 - loss: 0.5308\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8157 - loss: 0.5307 - val_AUC: 0.8016 - val_loss: 0.5461 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8187 - loss: 0.5268 - val_AUC: 0.8013 - val_loss: 0.5473 - learning_rate: 1.2500e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8199 - loss: 0.5249 - val_AUC: 0.8016 - val_loss: 0.5468 - learning_rate: 1.2500e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8178 - loss: 0.5278 - val_AUC: 0.8022 - val_loss: 0.5460 - learning_rate: 1.2500e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8177 - loss: 0.5279 - val_AUC: 0.8025 - val_loss: 0.5460 - learning_rate: 1.2500e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8198 - loss: 0.5251 - val_AUC: 0.8029 - val_loss: 0.5452 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8200 - loss: 0.5258 - val_AUC: 0.8023 - val_loss: 0.5458 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8212 - loss: 0.5229 - val_AUC: 0.8032 - val_loss: 0.5452 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8185 - loss: 0.5273 - val_AUC: 0.8024 - val_loss: 0.5465 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8216 - loss: 0.5230 - val_AUC: 0.8022 - val_loss: 0.5468 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8215 - loss: 0.5233 - val_AUC: 0.8020 - val_loss: 0.5459 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8223 - loss: 0.5225 - val_AUC: 0.8018 - val_loss: 0.5467 - learning_rate: 1.2500e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8228 - loss: 0.5215\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8229 - loss: 0.5215 - val_AUC: 0.8019 - val_loss: 0.5469 - learning_rate: 1.2500e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8232 - loss: 0.5216 - val_AUC: 0.8021 - val_loss: 0.5469 - learning_rate: 6.2500e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8241 - loss: 0.5208 - val_AUC: 0.8023 - val_loss: 0.5466 - learning_rate: 6.2500e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8194 - loss: 0.5265 - val_AUC: 0.8020 - val_loss: 0.5473 - learning_rate: 6.2500e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8226 - loss: 0.5220 - val_AUC: 0.8021 - val_loss: 0.5473 - learning_rate: 6.2500e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8242 - loss: 0.5203\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8242 - loss: 0.5202 - val_AUC: 0.8022 - val_loss: 0.5472 - learning_rate: 6.2500e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 8/20 | HP Config 2 | Seed 300\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - AUC: 0.5808 - loss: 0.7274 - val_AUC: 0.6994 - val_loss: 0.6363 - learning_rate: 5.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6627 - loss: 0.6620 - val_AUC: 0.7223 - val_loss: 0.6199 - learning_rate: 5.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6987 - loss: 0.6391 - val_AUC: 0.7357 - val_loss: 0.6106 - learning_rate: 5.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7132 - loss: 0.6287 - val_AUC: 0.7443 - val_loss: 0.6025 - learning_rate: 5.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7299 - loss: 0.6156 - val_AUC: 0.7508 - val_loss: 0.5957 - learning_rate: 5.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7373 - loss: 0.6092 - val_AUC: 0.7543 - val_loss: 0.5928 - learning_rate: 5.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7424 - loss: 0.6048 - val_AUC: 0.7614 - val_loss: 0.5862 - learning_rate: 5.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7497 - loss: 0.5992 - val_AUC: 0.7660 - val_loss: 0.5816 - learning_rate: 5.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7517 - loss: 0.5972 - val_AUC: 0.7676 - val_loss: 0.5799 - learning_rate: 5.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7582 - loss: 0.5907 - val_AUC: 0.7706 - val_loss: 0.5770 - learning_rate: 5.0000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7616 - loss: 0.5880 - val_AUC: 0.7724 - val_loss: 0.5749 - learning_rate: 5.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7661 - loss: 0.5841 - val_AUC: 0.7742 - val_loss: 0.5732 - learning_rate: 5.0000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7685 - loss: 0.5814 - val_AUC: 0.7766 - val_loss: 0.5711 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7711 - loss: 0.5793 - val_AUC: 0.7781 - val_loss: 0.5691 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7737 - loss: 0.5772 - val_AUC: 0.7796 - val_loss: 0.5674 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7770 - loss: 0.5735 - val_AUC: 0.7804 - val_loss: 0.5662 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7767 - loss: 0.5735 - val_AUC: 0.7797 - val_loss: 0.5670 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7793 - loss: 0.5711 - val_AUC: 0.7830 - val_loss: 0.5645 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7811 - loss: 0.5692 - val_AUC: 0.7831 - val_loss: 0.5634 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7820 - loss: 0.5688 - val_AUC: 0.7842 - val_loss: 0.5628 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7824 - loss: 0.5678 - val_AUC: 0.7826 - val_loss: 0.5652 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7852 - loss: 0.5654 - val_AUC: 0.7849 - val_loss: 0.5625 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7870 - loss: 0.5635 - val_AUC: 0.7872 - val_loss: 0.5600 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7872 - loss: 0.5637 - val_AUC: 0.7860 - val_loss: 0.5613 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7876 - loss: 0.5619 - val_AUC: 0.7862 - val_loss: 0.5608 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7896 - loss: 0.5606 - val_AUC: 0.7867 - val_loss: 0.5606 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7909 - loss: 0.5596 - val_AUC: 0.7876 - val_loss: 0.5600 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7904 - loss: 0.5593 - val_AUC: 0.7876 - val_loss: 0.5594 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7932 - loss: 0.5566 - val_AUC: 0.7882 - val_loss: 0.5595 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7939 - loss: 0.5556 - val_AUC: 0.7892 - val_loss: 0.5581 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7949 - loss: 0.5543 - val_AUC: 0.7882 - val_loss: 0.5595 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7946 - loss: 0.5554 - val_AUC: 0.7898 - val_loss: 0.5578 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7965 - loss: 0.5526 - val_AUC: 0.7914 - val_loss: 0.5565 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7952 - loss: 0.5538 - val_AUC: 0.7913 - val_loss: 0.5558 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7977 - loss: 0.5516 - val_AUC: 0.7901 - val_loss: 0.5573 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7975 - loss: 0.5513 - val_AUC: 0.7924 - val_loss: 0.5550 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7984 - loss: 0.5505 - val_AUC: 0.7915 - val_loss: 0.5565 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7987 - loss: 0.5507 - val_AUC: 0.7925 - val_loss: 0.5550 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7996 - loss: 0.5492 - val_AUC: 0.7941 - val_loss: 0.5532 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8013 - loss: 0.5483 - val_AUC: 0.7930 - val_loss: 0.5547 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8023 - loss: 0.5461 - val_AUC: 0.7937 - val_loss: 0.5541 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8009 - loss: 0.5471 - val_AUC: 0.7936 - val_loss: 0.5538 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8034 - loss: 0.5457 - val_AUC: 0.7936 - val_loss: 0.5549 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8020 - loss: 0.5457 - val_AUC: 0.7943 - val_loss: 0.5534 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8019 - loss: 0.5467 - val_AUC: 0.7954 - val_loss: 0.5522 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8048 - loss: 0.5434 - val_AUC: 0.7938 - val_loss: 0.5537 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8070 - loss: 0.5417 - val_AUC: 0.7935 - val_loss: 0.5542 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8056 - loss: 0.5430 - val_AUC: 0.7951 - val_loss: 0.5522 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8066 - loss: 0.5418 - val_AUC: 0.7940 - val_loss: 0.5530 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8091 - loss: 0.5389 - val_AUC: 0.7958 - val_loss: 0.5522 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8086 - loss: 0.5385 - val_AUC: 0.7957 - val_loss: 0.5520 - learning_rate: 5.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8076 - loss: 0.5404 - val_AUC: 0.7962 - val_loss: 0.5517 - learning_rate: 5.0000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8097 - loss: 0.5385 - val_AUC: 0.7958 - val_loss: 0.5523 - learning_rate: 5.0000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8102 - loss: 0.5370 - val_AUC: 0.7963 - val_loss: 0.5517 - learning_rate: 5.0000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8100 - loss: 0.5371 - val_AUC: 0.7962 - val_loss: 0.5516 - learning_rate: 5.0000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8112 - loss: 0.5362 - val_AUC: 0.7962 - val_loss: 0.5515 - learning_rate: 5.0000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m348/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8107 - loss: 0.5371\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8106 - loss: 0.5371 - val_AUC: 0.7960 - val_loss: 0.5516 - learning_rate: 5.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8145 - loss: 0.5335 - val_AUC: 0.7971 - val_loss: 0.5505 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8161 - loss: 0.5304 - val_AUC: 0.7977 - val_loss: 0.5496 - learning_rate: 2.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8152 - loss: 0.5304 - val_AUC: 0.7978 - val_loss: 0.5496 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8181 - loss: 0.5276 - val_AUC: 0.7991 - val_loss: 0.5486 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8164 - loss: 0.5304 - val_AUC: 0.7997 - val_loss: 0.5482 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8183 - loss: 0.5279 - val_AUC: 0.8002 - val_loss: 0.5477 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8182 - loss: 0.5276 - val_AUC: 0.7995 - val_loss: 0.5491 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8203 - loss: 0.5245 - val_AUC: 0.7996 - val_loss: 0.5485 - learning_rate: 2.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8196 - loss: 0.5259 - val_AUC: 0.7994 - val_loss: 0.5485 - learning_rate: 2.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8184 - loss: 0.5267 - val_AUC: 0.7998 - val_loss: 0.5479 - learning_rate: 2.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m341/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.8193 - loss: 0.5264\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8193 - loss: 0.5264 - val_AUC: 0.8000 - val_loss: 0.5490 - learning_rate: 2.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8221 - loss: 0.5228 - val_AUC: 0.8005 - val_loss: 0.5478 - learning_rate: 1.2500e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8209 - loss: 0.5237 - val_AUC: 0.8000 - val_loss: 0.5478 - learning_rate: 1.2500e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8231 - loss: 0.5211 - val_AUC: 0.8005 - val_loss: 0.5476 - learning_rate: 1.2500e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8224 - loss: 0.5228 - val_AUC: 0.7999 - val_loss: 0.5484 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8247 - loss: 0.5198 - val_AUC: 0.8008 - val_loss: 0.5475 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8230 - loss: 0.5216 - val_AUC: 0.8004 - val_loss: 0.5479 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8252 - loss: 0.5193 - val_AUC: 0.8011 - val_loss: 0.5473 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8219 - loss: 0.5228 - val_AUC: 0.8009 - val_loss: 0.5473 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8241 - loss: 0.5206 - val_AUC: 0.8005 - val_loss: 0.5477 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8233 - loss: 0.5214 - val_AUC: 0.8007 - val_loss: 0.5476 - learning_rate: 1.2500e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8237 - loss: 0.5204 - val_AUC: 0.8004 - val_loss: 0.5480 - learning_rate: 1.2500e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m347/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8225 - loss: 0.5227\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8225 - loss: 0.5226 - val_AUC: 0.8005 - val_loss: 0.5476 - learning_rate: 1.2500e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8239 - loss: 0.5197 - val_AUC: 0.8007 - val_loss: 0.5479 - learning_rate: 6.2500e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8222 - loss: 0.5220 - val_AUC: 0.8010 - val_loss: 0.5475 - learning_rate: 6.2500e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8262 - loss: 0.5178 - val_AUC: 0.8010 - val_loss: 0.5478 - learning_rate: 6.2500e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8261 - loss: 0.5173 - val_AUC: 0.8011 - val_loss: 0.5476 - learning_rate: 6.2500e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8255 - loss: 0.5183 - val_AUC: 0.8014 - val_loss: 0.5473 - learning_rate: 6.2500e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8270 - loss: 0.5162 - val_AUC: 0.8015 - val_loss: 0.5471 - learning_rate: 6.2500e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8250 - loss: 0.5196 - val_AUC: 0.8012 - val_loss: 0.5475 - learning_rate: 6.2500e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8257 - loss: 0.5186 - val_AUC: 0.8015 - val_loss: 0.5471 - learning_rate: 6.2500e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8277 - loss: 0.5155 - val_AUC: 0.8009 - val_loss: 0.5480 - learning_rate: 6.2500e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8248 - loss: 0.5191 - val_AUC: 0.8009 - val_loss: 0.5478 - learning_rate: 6.2500e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8259 - loss: 0.5175 - val_AUC: 0.8011 - val_loss: 0.5477 - learning_rate: 6.2500e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8271 - loss: 0.5161 - val_AUC: 0.8009 - val_loss: 0.5482 - learning_rate: 6.2500e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8262 - loss: 0.5183\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8262 - loss: 0.5183 - val_AUC: 0.8009 - val_loss: 0.5480 - learning_rate: 6.2500e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8258 - loss: 0.5180 - val_AUC: 0.8009 - val_loss: 0.5480 - learning_rate: 3.1250e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8259 - loss: 0.5181 - val_AUC: 0.8008 - val_loss: 0.5480 - learning_rate: 3.1250e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8290 - loss: 0.5136 - val_AUC: 0.8009 - val_loss: 0.5483 - learning_rate: 3.1250e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8235 - loss: 0.5204 - val_AUC: 0.8010 - val_loss: 0.5476 - learning_rate: 3.1250e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m342/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8264 - loss: 0.5172\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8264 - loss: 0.5171 - val_AUC: 0.8011 - val_loss: 0.5477 - learning_rate: 3.1250e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 9/20 | HP Config 3 | Seed 0\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - AUC: 0.6216 - loss: 0.7226 - val_AUC: 0.7044 - val_loss: 0.6466 - learning_rate: 0.0020\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7211 - loss: 0.6316 - val_AUC: 0.7379 - val_loss: 0.6174 - learning_rate: 0.0020\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7440 - loss: 0.6134 - val_AUC: 0.7396 - val_loss: 0.6133 - learning_rate: 0.0020\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7551 - loss: 0.6035 - val_AUC: 0.7651 - val_loss: 0.5934 - learning_rate: 0.0020\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7635 - loss: 0.5962 - val_AUC: 0.7693 - val_loss: 0.5942 - learning_rate: 0.0020\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7664 - loss: 0.5929 - val_AUC: 0.7755 - val_loss: 0.5851 - learning_rate: 0.0020\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7721 - loss: 0.5869 - val_AUC: 0.7790 - val_loss: 0.5798 - learning_rate: 0.0020\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7734 - loss: 0.5850 - val_AUC: 0.7788 - val_loss: 0.5808 - learning_rate: 0.0020\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7780 - loss: 0.5805 - val_AUC: 0.7799 - val_loss: 0.5785 - learning_rate: 0.0020\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7810 - loss: 0.5769 - val_AUC: 0.7820 - val_loss: 0.5734 - learning_rate: 0.0020\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7805 - loss: 0.5774 - val_AUC: 0.7832 - val_loss: 0.5723 - learning_rate: 0.0020\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7864 - loss: 0.5713 - val_AUC: 0.7841 - val_loss: 0.5727 - learning_rate: 0.0020\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7882 - loss: 0.5695 - val_AUC: 0.7863 - val_loss: 0.5695 - learning_rate: 0.0020\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7883 - loss: 0.5696 - val_AUC: 0.7860 - val_loss: 0.5704 - learning_rate: 0.0020\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7897 - loss: 0.5677 - val_AUC: 0.7868 - val_loss: 0.5695 - learning_rate: 0.0020\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7921 - loss: 0.5651 - val_AUC: 0.7857 - val_loss: 0.5701 - learning_rate: 0.0020\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.7942 - loss: 0.5635 - val_AUC: 0.7869 - val_loss: 0.5697 - learning_rate: 0.0020\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.7965 - loss: 0.5610 - val_AUC: 0.7887 - val_loss: 0.5696 - learning_rate: 0.0020\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7979 - loss: 0.5593 - val_AUC: 0.7863 - val_loss: 0.5696 - learning_rate: 0.0020\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7981 - loss: 0.5596 - val_AUC: 0.7894 - val_loss: 0.5681 - learning_rate: 0.0020\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7997 - loss: 0.5576 - val_AUC: 0.7879 - val_loss: 0.5684 - learning_rate: 0.0020\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8013 - loss: 0.5560 - val_AUC: 0.7881 - val_loss: 0.5719 - learning_rate: 0.0020\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8040 - loss: 0.5539 - val_AUC: 0.7872 - val_loss: 0.5699 - learning_rate: 0.0020\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8029 - loss: 0.5545 - val_AUC: 0.7877 - val_loss: 0.5738 - learning_rate: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8046 - loss: 0.5526 - val_AUC: 0.7912 - val_loss: 0.5666 - learning_rate: 0.0020\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8082 - loss: 0.5488 - val_AUC: 0.7899 - val_loss: 0.5689 - learning_rate: 0.0020\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8053 - loss: 0.5522 - val_AUC: 0.7910 - val_loss: 0.5680 - learning_rate: 0.0020\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.8119 - loss: 0.5459 - val_AUC: 0.7920 - val_loss: 0.5674 - learning_rate: 0.0020\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.8103 - loss: 0.5468 - val_AUC: 0.7889 - val_loss: 0.5743 - learning_rate: 0.0020\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.8108 - loss: 0.5466 - val_AUC: 0.7900 - val_loss: 0.5710 - learning_rate: 0.0020\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - AUC: 0.8109 - loss: 0.5464 - val_AUC: 0.7894 - val_loss: 0.5758 - learning_rate: 0.0020\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.8127 - loss: 0.5450 - val_AUC: 0.7884 - val_loss: 0.5750 - learning_rate: 0.0020\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.8159 - loss: 0.5415\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.8159 - loss: 0.5415 - val_AUC: 0.7899 - val_loss: 0.5750 - learning_rate: 0.0020\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.8212 - loss: 0.5355 - val_AUC: 0.7945 - val_loss: 0.5713 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8258 - loss: 0.5295 - val_AUC: 0.7950 - val_loss: 0.5720 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.8254 - loss: 0.5298 - val_AUC: 0.7950 - val_loss: 0.5720 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.8292 - loss: 0.5259 - val_AUC: 0.7933 - val_loss: 0.5770 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.8299 - loss: 0.5246 - val_AUC: 0.7961 - val_loss: 0.5694 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8320 - loss: 0.5221 - val_AUC: 0.7946 - val_loss: 0.5752 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.8314 - loss: 0.5223 - val_AUC: 0.7946 - val_loss: 0.5763 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.8333 - loss: 0.5199 - val_AUC: 0.7941 - val_loss: 0.5750 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8336 - loss: 0.5193 - val_AUC: 0.7936 - val_loss: 0.5762 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m349/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8340 - loss: 0.5188\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8340 - loss: 0.5187 - val_AUC: 0.7928 - val_loss: 0.5840 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8369 - loss: 0.5154 - val_AUC: 0.7956 - val_loss: 0.5713 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8378 - loss: 0.5137 - val_AUC: 0.7958 - val_loss: 0.5792 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8406 - loss: 0.5106 - val_AUC: 0.7953 - val_loss: 0.5771 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8436 - loss: 0.5057 - val_AUC: 0.7957 - val_loss: 0.5785 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m349/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8444 - loss: 0.5049\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8445 - loss: 0.5049 - val_AUC: 0.7952 - val_loss: 0.5785 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 10/20 | HP Config 3 | Seed 100\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - AUC: 0.6247 - loss: 0.7138 - val_AUC: 0.7414 - val_loss: 0.6438 - learning_rate: 0.0020\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7230 - loss: 0.6327 - val_AUC: 0.7547 - val_loss: 0.6051 - learning_rate: 0.0020\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7418 - loss: 0.6162 - val_AUC: 0.7635 - val_loss: 0.5923 - learning_rate: 0.0020\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7536 - loss: 0.6065 - val_AUC: 0.7698 - val_loss: 0.5865 - learning_rate: 0.0020\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7598 - loss: 0.6000 - val_AUC: 0.7726 - val_loss: 0.5828 - learning_rate: 0.0020\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7665 - loss: 0.5928 - val_AUC: 0.7769 - val_loss: 0.5781 - learning_rate: 0.0020\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7703 - loss: 0.5894 - val_AUC: 0.7808 - val_loss: 0.5740 - learning_rate: 0.0020\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7744 - loss: 0.5848 - val_AUC: 0.7823 - val_loss: 0.5730 - learning_rate: 0.0020\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7764 - loss: 0.5835 - val_AUC: 0.7821 - val_loss: 0.5735 - learning_rate: 0.0020\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7813 - loss: 0.5776 - val_AUC: 0.7846 - val_loss: 0.5697 - learning_rate: 0.0020\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7822 - loss: 0.5767 - val_AUC: 0.7840 - val_loss: 0.5714 - learning_rate: 0.0020\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7842 - loss: 0.5744 - val_AUC: 0.7880 - val_loss: 0.5675 - learning_rate: 0.0020\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7895 - loss: 0.5693 - val_AUC: 0.7883 - val_loss: 0.5675 - learning_rate: 0.0020\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7884 - loss: 0.5703 - val_AUC: 0.7900 - val_loss: 0.5647 - learning_rate: 0.0020\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7909 - loss: 0.5666 - val_AUC: 0.7880 - val_loss: 0.5670 - learning_rate: 0.0020\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7924 - loss: 0.5659 - val_AUC: 0.7895 - val_loss: 0.5666 - learning_rate: 0.0020\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7937 - loss: 0.5650 - val_AUC: 0.7884 - val_loss: 0.5668 - learning_rate: 0.0020\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7976 - loss: 0.5605 - val_AUC: 0.7913 - val_loss: 0.5654 - learning_rate: 0.0020\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7965 - loss: 0.5618 - val_AUC: 0.7913 - val_loss: 0.5649 - learning_rate: 0.0020\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8005 - loss: 0.5570 - val_AUC: 0.7931 - val_loss: 0.5635 - learning_rate: 0.0020\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8017 - loss: 0.5554 - val_AUC: 0.7927 - val_loss: 0.5650 - learning_rate: 0.0020\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8036 - loss: 0.5545 - val_AUC: 0.7920 - val_loss: 0.5649 - learning_rate: 0.0020\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8037 - loss: 0.5540 - val_AUC: 0.7924 - val_loss: 0.5665 - learning_rate: 0.0020\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8033 - loss: 0.5545 - val_AUC: 0.7955 - val_loss: 0.5618 - learning_rate: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8083 - loss: 0.5496 - val_AUC: 0.7942 - val_loss: 0.5662 - learning_rate: 0.0020\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8084 - loss: 0.5496 - val_AUC: 0.7941 - val_loss: 0.5659 - learning_rate: 0.0020\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8095 - loss: 0.5490 - val_AUC: 0.7939 - val_loss: 0.5692 - learning_rate: 0.0020\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8115 - loss: 0.5472 - val_AUC: 0.7947 - val_loss: 0.5667 - learning_rate: 0.0020\n",
      "Epoch 29/200\n",
      "\u001b[1m348/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8115 - loss: 0.5471\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8116 - loss: 0.5471 - val_AUC: 0.7951 - val_loss: 0.5658 - learning_rate: 0.0020\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8170 - loss: 0.5408 - val_AUC: 0.7985 - val_loss: 0.5637 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8225 - loss: 0.5325 - val_AUC: 0.7980 - val_loss: 0.5634 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8214 - loss: 0.5346 - val_AUC: 0.7956 - val_loss: 0.5683 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8233 - loss: 0.5318 - val_AUC: 0.7957 - val_loss: 0.5669 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8255 - loss: 0.5295 - val_AUC: 0.7948 - val_loss: 0.5727 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m348/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8274 - loss: 0.5273\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8274 - loss: 0.5273 - val_AUC: 0.7938 - val_loss: 0.5687 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8312 - loss: 0.5224 - val_AUC: 0.7967 - val_loss: 0.5697 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8334 - loss: 0.5191 - val_AUC: 0.7973 - val_loss: 0.5690 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8346 - loss: 0.5179 - val_AUC: 0.7969 - val_loss: 0.5702 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8324 - loss: 0.5200 - val_AUC: 0.7969 - val_loss: 0.5736 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8358 - loss: 0.5158\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8358 - loss: 0.5158 - val_AUC: 0.7978 - val_loss: 0.5698 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 11/20 | HP Config 3 | Seed 200\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - AUC: 0.6170 - loss: 0.7242 - val_AUC: 0.7316 - val_loss: 0.6264 - learning_rate: 0.0020\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7192 - loss: 0.6353 - val_AUC: 0.7511 - val_loss: 0.6042 - learning_rate: 0.0020\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7417 - loss: 0.6158 - val_AUC: 0.7605 - val_loss: 0.5958 - learning_rate: 0.0020\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7547 - loss: 0.6045 - val_AUC: 0.7680 - val_loss: 0.5886 - learning_rate: 0.0020\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7588 - loss: 0.6004 - val_AUC: 0.7707 - val_loss: 0.5880 - learning_rate: 0.0020\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7634 - loss: 0.5954 - val_AUC: 0.7762 - val_loss: 0.5825 - learning_rate: 0.0020\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7687 - loss: 0.5897 - val_AUC: 0.7771 - val_loss: 0.5821 - learning_rate: 0.0020\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7711 - loss: 0.5873 - val_AUC: 0.7776 - val_loss: 0.5813 - learning_rate: 0.0020\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7764 - loss: 0.5831 - val_AUC: 0.7798 - val_loss: 0.5791 - learning_rate: 0.0020\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7775 - loss: 0.5810 - val_AUC: 0.7844 - val_loss: 0.5721 - learning_rate: 0.0020\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7807 - loss: 0.5778 - val_AUC: 0.7870 - val_loss: 0.5723 - learning_rate: 0.0020\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7825 - loss: 0.5760 - val_AUC: 0.7868 - val_loss: 0.5735 - learning_rate: 0.0020\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7831 - loss: 0.5760 - val_AUC: 0.7884 - val_loss: 0.5704 - learning_rate: 0.0020\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7856 - loss: 0.5728 - val_AUC: 0.7865 - val_loss: 0.5730 - learning_rate: 0.0020\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7882 - loss: 0.5702 - val_AUC: 0.7863 - val_loss: 0.5731 - learning_rate: 0.0020\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7884 - loss: 0.5694 - val_AUC: 0.7888 - val_loss: 0.5696 - learning_rate: 0.0020\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7921 - loss: 0.5663 - val_AUC: 0.7920 - val_loss: 0.5658 - learning_rate: 0.0020\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7941 - loss: 0.5640 - val_AUC: 0.7910 - val_loss: 0.5681 - learning_rate: 0.0020\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7951 - loss: 0.5628 - val_AUC: 0.7926 - val_loss: 0.5660 - learning_rate: 0.0020\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7961 - loss: 0.5613 - val_AUC: 0.7896 - val_loss: 0.5710 - learning_rate: 0.0020\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7990 - loss: 0.5591 - val_AUC: 0.7907 - val_loss: 0.5713 - learning_rate: 0.0020\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7990 - loss: 0.5591 - val_AUC: 0.7919 - val_loss: 0.5695 - learning_rate: 0.0020\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8031 - loss: 0.5551 - val_AUC: 0.7908 - val_loss: 0.5692 - learning_rate: 0.0020\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8030 - loss: 0.5556 - val_AUC: 0.7937 - val_loss: 0.5664 - learning_rate: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8041 - loss: 0.5542 - val_AUC: 0.7916 - val_loss: 0.5658 - learning_rate: 0.0020\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8058 - loss: 0.5532 - val_AUC: 0.7924 - val_loss: 0.5712 - learning_rate: 0.0020\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8062 - loss: 0.5520 - val_AUC: 0.7897 - val_loss: 0.5729 - learning_rate: 0.0020\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8062 - loss: 0.5527 - val_AUC: 0.7908 - val_loss: 0.5710 - learning_rate: 0.0020\n",
      "Epoch 29/200\n",
      "\u001b[1m348/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8103 - loss: 0.5472\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8103 - loss: 0.5472 - val_AUC: 0.7908 - val_loss: 0.5690 - learning_rate: 0.0020\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8155 - loss: 0.5431 - val_AUC: 0.7960 - val_loss: 0.5621 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8198 - loss: 0.5357 - val_AUC: 0.7940 - val_loss: 0.5658 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8197 - loss: 0.5363 - val_AUC: 0.7950 - val_loss: 0.5662 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8211 - loss: 0.5338 - val_AUC: 0.7950 - val_loss: 0.5636 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8226 - loss: 0.5339 - val_AUC: 0.7948 - val_loss: 0.5655 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m347/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8226 - loss: 0.5318\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8227 - loss: 0.5317 - val_AUC: 0.7948 - val_loss: 0.5666 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8280 - loss: 0.5258 - val_AUC: 0.7978 - val_loss: 0.5627 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8300 - loss: 0.5229 - val_AUC: 0.7970 - val_loss: 0.5643 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8318 - loss: 0.5202 - val_AUC: 0.7965 - val_loss: 0.5663 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8333 - loss: 0.5189 - val_AUC: 0.7964 - val_loss: 0.5667 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8355 - loss: 0.5158 - val_AUC: 0.7966 - val_loss: 0.5644 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8354 - loss: 0.5168\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8354 - loss: 0.5168 - val_AUC: 0.7969 - val_loss: 0.5644 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8370 - loss: 0.5125 - val_AUC: 0.7971 - val_loss: 0.5657 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8408 - loss: 0.5090 - val_AUC: 0.7965 - val_loss: 0.5661 - learning_rate: 2.5000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8383 - loss: 0.5118 - val_AUC: 0.7959 - val_loss: 0.5671 - learning_rate: 2.5000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8387 - loss: 0.5113 - val_AUC: 0.7965 - val_loss: 0.5678 - learning_rate: 2.5000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m348/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8400 - loss: 0.5088\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8401 - loss: 0.5087 - val_AUC: 0.7945 - val_loss: 0.5707 - learning_rate: 2.5000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 12/20 | HP Config 3 | Seed 300\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - AUC: 0.6251 - loss: 0.7119 - val_AUC: 0.7204 - val_loss: 0.6379 - learning_rate: 0.0020\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7241 - loss: 0.6311 - val_AUC: 0.7474 - val_loss: 0.6080 - learning_rate: 0.0020\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.7479 - loss: 0.6108 - val_AUC: 0.7539 - val_loss: 0.6027 - learning_rate: 0.0020\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.7574 - loss: 0.6019 - val_AUC: 0.7613 - val_loss: 0.5946 - learning_rate: 0.0020\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.7631 - loss: 0.5964 - val_AUC: 0.7662 - val_loss: 0.5907 - learning_rate: 0.0020\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7680 - loss: 0.5916 - val_AUC: 0.7707 - val_loss: 0.5856 - learning_rate: 0.0020\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7723 - loss: 0.5878 - val_AUC: 0.7733 - val_loss: 0.5832 - learning_rate: 0.0020\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7761 - loss: 0.5835 - val_AUC: 0.7754 - val_loss: 0.5827 - learning_rate: 0.0020\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7787 - loss: 0.5793 - val_AUC: 0.7763 - val_loss: 0.5813 - learning_rate: 0.0020\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7813 - loss: 0.5771 - val_AUC: 0.7802 - val_loss: 0.5752 - learning_rate: 0.0020\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7867 - loss: 0.5714 - val_AUC: 0.7803 - val_loss: 0.5797 - learning_rate: 0.0020\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.7877 - loss: 0.5708 - val_AUC: 0.7838 - val_loss: 0.5707 - learning_rate: 0.0020\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7883 - loss: 0.5692 - val_AUC: 0.7817 - val_loss: 0.5746 - learning_rate: 0.0020\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7897 - loss: 0.5683 - val_AUC: 0.7858 - val_loss: 0.5700 - learning_rate: 0.0020\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7900 - loss: 0.5678 - val_AUC: 0.7857 - val_loss: 0.5700 - learning_rate: 0.0020\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7944 - loss: 0.5634 - val_AUC: 0.7835 - val_loss: 0.5752 - learning_rate: 0.0020\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7969 - loss: 0.5606 - val_AUC: 0.7861 - val_loss: 0.5701 - learning_rate: 0.0020\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7981 - loss: 0.5591 - val_AUC: 0.7874 - val_loss: 0.5701 - learning_rate: 0.0020\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8008 - loss: 0.5566 - val_AUC: 0.7883 - val_loss: 0.5692 - learning_rate: 0.0020\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7983 - loss: 0.5596 - val_AUC: 0.7899 - val_loss: 0.5656 - learning_rate: 0.0020\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8010 - loss: 0.5559 - val_AUC: 0.7896 - val_loss: 0.5694 - learning_rate: 0.0020\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8038 - loss: 0.5535 - val_AUC: 0.7902 - val_loss: 0.5672 - learning_rate: 0.0020\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8039 - loss: 0.5535 - val_AUC: 0.7907 - val_loss: 0.5664 - learning_rate: 0.0020\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8061 - loss: 0.5506 - val_AUC: 0.7909 - val_loss: 0.5687 - learning_rate: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8077 - loss: 0.5496 - val_AUC: 0.7914 - val_loss: 0.5692 - learning_rate: 0.0020\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8094 - loss: 0.5480 - val_AUC: 0.7935 - val_loss: 0.5666 - learning_rate: 0.0020\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8085 - loss: 0.5487 - val_AUC: 0.7912 - val_loss: 0.5672 - learning_rate: 0.0020\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8114 - loss: 0.5453 - val_AUC: 0.7924 - val_loss: 0.5718 - learning_rate: 0.0020\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8131 - loss: 0.5440 - val_AUC: 0.7933 - val_loss: 0.5677 - learning_rate: 0.0020\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8141 - loss: 0.5433 - val_AUC: 0.7899 - val_loss: 0.5724 - learning_rate: 0.0020\n",
      "Epoch 31/200\n",
      "\u001b[1m349/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8150 - loss: 0.5424\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8150 - loss: 0.5424 - val_AUC: 0.7921 - val_loss: 0.5712 - learning_rate: 0.0020\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8205 - loss: 0.5359 - val_AUC: 0.7957 - val_loss: 0.5663 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8230 - loss: 0.5329 - val_AUC: 0.7962 - val_loss: 0.5672 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8246 - loss: 0.5308 - val_AUC: 0.7960 - val_loss: 0.5678 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8283 - loss: 0.5253 - val_AUC: 0.7954 - val_loss: 0.5690 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8297 - loss: 0.5238 - val_AUC: 0.7954 - val_loss: 0.5698 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8292 - loss: 0.5248 - val_AUC: 0.7943 - val_loss: 0.5737 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8328 - loss: 0.5204\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8328 - loss: 0.5204 - val_AUC: 0.7946 - val_loss: 0.5721 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8340 - loss: 0.5179 - val_AUC: 0.7963 - val_loss: 0.5685 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8372 - loss: 0.5136 - val_AUC: 0.7957 - val_loss: 0.5722 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8389 - loss: 0.5106 - val_AUC: 0.7955 - val_loss: 0.5727 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8379 - loss: 0.5134 - val_AUC: 0.7944 - val_loss: 0.5728 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8396 - loss: 0.5103 - val_AUC: 0.7960 - val_loss: 0.5733 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m348/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8410 - loss: 0.5086\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8410 - loss: 0.5086 - val_AUC: 0.7949 - val_loss: 0.5741 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8463 - loss: 0.5009 - val_AUC: 0.7956 - val_loss: 0.5732 - learning_rate: 2.5000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.8439 - loss: 0.5045 - val_AUC: 0.7961 - val_loss: 0.5745 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8465 - loss: 0.5013 - val_AUC: 0.7961 - val_loss: 0.5740 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8433 - loss: 0.5050 - val_AUC: 0.7950 - val_loss: 0.5741 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8465 - loss: 0.5008\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8465 - loss: 0.5008 - val_AUC: 0.7959 - val_loss: 0.5749 - learning_rate: 2.5000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 13/20 | HP Config 4 | Seed 0\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - AUC: 0.5496 - loss: 0.7446 - val_AUC: 0.6887 - val_loss: 0.6505 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.6615 - loss: 0.6655 - val_AUC: 0.7199 - val_loss: 0.6326 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.6980 - loss: 0.6427 - val_AUC: 0.7351 - val_loss: 0.6203 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7127 - loss: 0.6318 - val_AUC: 0.7463 - val_loss: 0.6116 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7284 - loss: 0.6212 - val_AUC: 0.7543 - val_loss: 0.6005 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7383 - loss: 0.6124 - val_AUC: 0.7571 - val_loss: 0.5975 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7465 - loss: 0.6056 - val_AUC: 0.7631 - val_loss: 0.5910 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7523 - loss: 0.6000 - val_AUC: 0.7683 - val_loss: 0.5843 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7548 - loss: 0.5982 - val_AUC: 0.7691 - val_loss: 0.5838 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7591 - loss: 0.5941 - val_AUC: 0.7689 - val_loss: 0.5841 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7599 - loss: 0.5936 - val_AUC: 0.7733 - val_loss: 0.5795 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7678 - loss: 0.5865 - val_AUC: 0.7755 - val_loss: 0.5774 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7665 - loss: 0.5871 - val_AUC: 0.7782 - val_loss: 0.5745 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7707 - loss: 0.5831 - val_AUC: 0.7779 - val_loss: 0.5748 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7747 - loss: 0.5799 - val_AUC: 0.7809 - val_loss: 0.5716 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7744 - loss: 0.5797 - val_AUC: 0.7847 - val_loss: 0.5675 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7749 - loss: 0.5790 - val_AUC: 0.7845 - val_loss: 0.5668 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7781 - loss: 0.5758 - val_AUC: 0.7853 - val_loss: 0.5671 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7802 - loss: 0.5738 - val_AUC: 0.7856 - val_loss: 0.5663 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7809 - loss: 0.5728 - val_AUC: 0.7870 - val_loss: 0.5653 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7829 - loss: 0.5704 - val_AUC: 0.7876 - val_loss: 0.5637 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7846 - loss: 0.5681 - val_AUC: 0.7868 - val_loss: 0.5640 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7860 - loss: 0.5672 - val_AUC: 0.7881 - val_loss: 0.5628 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7854 - loss: 0.5672 - val_AUC: 0.7900 - val_loss: 0.5611 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7869 - loss: 0.5663 - val_AUC: 0.7909 - val_loss: 0.5598 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7907 - loss: 0.5626 - val_AUC: 0.7917 - val_loss: 0.5587 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7906 - loss: 0.5618 - val_AUC: 0.7904 - val_loss: 0.5613 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7900 - loss: 0.5633 - val_AUC: 0.7918 - val_loss: 0.5618 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7928 - loss: 0.5602 - val_AUC: 0.7927 - val_loss: 0.5579 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7903 - loss: 0.5621 - val_AUC: 0.7925 - val_loss: 0.5579 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7952 - loss: 0.5570 - val_AUC: 0.7933 - val_loss: 0.5572 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7959 - loss: 0.5559 - val_AUC: 0.7939 - val_loss: 0.5576 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7945 - loss: 0.5582 - val_AUC: 0.7931 - val_loss: 0.5581 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7963 - loss: 0.5555 - val_AUC: 0.7949 - val_loss: 0.5581 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7942 - loss: 0.5585 - val_AUC: 0.7941 - val_loss: 0.5574 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7972 - loss: 0.5550 - val_AUC: 0.7928 - val_loss: 0.5590 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.7971 - loss: 0.5548 - val_AUC: 0.7966 - val_loss: 0.5538 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7983 - loss: 0.5533 - val_AUC: 0.7955 - val_loss: 0.5563 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7995 - loss: 0.5524 - val_AUC: 0.7968 - val_loss: 0.5555 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7993 - loss: 0.5519 - val_AUC: 0.7968 - val_loss: 0.5556 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8023 - loss: 0.5497 - val_AUC: 0.7966 - val_loss: 0.5561 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8031 - loss: 0.5492 - val_AUC: 0.7973 - val_loss: 0.5540 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8023 - loss: 0.5498 - val_AUC: 0.7963 - val_loss: 0.5571 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8038 - loss: 0.5477 - val_AUC: 0.7983 - val_loss: 0.5537 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8029 - loss: 0.5487 - val_AUC: 0.7982 - val_loss: 0.5548 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8022 - loss: 0.5498 - val_AUC: 0.7973 - val_loss: 0.5552 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8038 - loss: 0.5482 - val_AUC: 0.7989 - val_loss: 0.5550 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8068 - loss: 0.5445 - val_AUC: 0.7993 - val_loss: 0.5531 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8073 - loss: 0.5449 - val_AUC: 0.7992 - val_loss: 0.5534 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8079 - loss: 0.5443 - val_AUC: 0.7991 - val_loss: 0.5569 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8089 - loss: 0.5432 - val_AUC: 0.8016 - val_loss: 0.5526 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8076 - loss: 0.5441 - val_AUC: 0.7993 - val_loss: 0.5557 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8107 - loss: 0.5412 - val_AUC: 0.8013 - val_loss: 0.5533 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8085 - loss: 0.5436 - val_AUC: 0.7997 - val_loss: 0.5568 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8092 - loss: 0.5427 - val_AUC: 0.8017 - val_loss: 0.5540 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8122 - loss: 0.5387 - val_AUC: 0.8010 - val_loss: 0.5532 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8120 - loss: 0.5404 - val_AUC: 0.8016 - val_loss: 0.5538 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8114 - loss: 0.5402 - val_AUC: 0.8028 - val_loss: 0.5515 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8144 - loss: 0.5366 - val_AUC: 0.8018 - val_loss: 0.5524 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8126 - loss: 0.5389 - val_AUC: 0.8020 - val_loss: 0.5537 - learning_rate: 0.0010\n",
      "Epoch 61/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8144 - loss: 0.5360 - val_AUC: 0.8014 - val_loss: 0.5565 - learning_rate: 0.0010\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8133 - loss: 0.5393 - val_AUC: 0.7997 - val_loss: 0.5584 - learning_rate: 0.0010\n",
      "Epoch 63/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8115 - loss: 0.5406\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8115 - loss: 0.5405 - val_AUC: 0.8014 - val_loss: 0.5556 - learning_rate: 0.0010\n",
      "Epoch 64/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8175 - loss: 0.5334 - val_AUC: 0.8033 - val_loss: 0.5540 - learning_rate: 5.0000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8199 - loss: 0.5305 - val_AUC: 0.8038 - val_loss: 0.5533 - learning_rate: 5.0000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8209 - loss: 0.5288 - val_AUC: 0.8052 - val_loss: 0.5509 - learning_rate: 5.0000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8206 - loss: 0.5294 - val_AUC: 0.8054 - val_loss: 0.5524 - learning_rate: 5.0000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8207 - loss: 0.5284 - val_AUC: 0.8051 - val_loss: 0.5529 - learning_rate: 5.0000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8236 - loss: 0.5242 - val_AUC: 0.8039 - val_loss: 0.5569 - learning_rate: 5.0000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8214 - loss: 0.5280 - val_AUC: 0.8051 - val_loss: 0.5509 - learning_rate: 5.0000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8232 - loss: 0.5256 - val_AUC: 0.8036 - val_loss: 0.5535 - learning_rate: 5.0000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - AUC: 0.8239 - loss: 0.5239\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8239 - loss: 0.5239 - val_AUC: 0.8034 - val_loss: 0.5580 - learning_rate: 5.0000e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8251 - loss: 0.5222 - val_AUC: 0.8046 - val_loss: 0.5546 - learning_rate: 2.5000e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8256 - loss: 0.5215 - val_AUC: 0.8048 - val_loss: 0.5535 - learning_rate: 2.5000e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8277 - loss: 0.5196 - val_AUC: 0.8056 - val_loss: 0.5528 - learning_rate: 2.5000e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8273 - loss: 0.5201 - val_AUC: 0.8054 - val_loss: 0.5535 - learning_rate: 2.5000e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8317 - loss: 0.5146 - val_AUC: 0.8053 - val_loss: 0.5525 - learning_rate: 2.5000e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8303 - loss: 0.5154 - val_AUC: 0.8061 - val_loss: 0.5539 - learning_rate: 2.5000e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8290 - loss: 0.5171 - val_AUC: 0.8055 - val_loss: 0.5528 - learning_rate: 2.5000e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8296 - loss: 0.5167 - val_AUC: 0.8054 - val_loss: 0.5524 - learning_rate: 2.5000e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8318 - loss: 0.5134 - val_AUC: 0.8062 - val_loss: 0.5515 - learning_rate: 2.5000e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8283 - loss: 0.5178 - val_AUC: 0.8066 - val_loss: 0.5510 - learning_rate: 2.5000e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8304 - loss: 0.5156 - val_AUC: 0.8066 - val_loss: 0.5528 - learning_rate: 2.5000e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8310 - loss: 0.5150 - val_AUC: 0.8061 - val_loss: 0.5529 - learning_rate: 2.5000e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8318 - loss: 0.5134 - val_AUC: 0.8064 - val_loss: 0.5528 - learning_rate: 2.5000e-04\n",
      "Epoch 86/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8320 - loss: 0.5131 - val_AUC: 0.8063 - val_loss: 0.5525 - learning_rate: 2.5000e-04\n",
      "Epoch 87/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8270 - loss: 0.5197 - val_AUC: 0.8069 - val_loss: 0.5514 - learning_rate: 2.5000e-04\n",
      "Epoch 88/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8304 - loss: 0.5159 - val_AUC: 0.8060 - val_loss: 0.5521 - learning_rate: 2.5000e-04\n",
      "Epoch 89/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8290 - loss: 0.5166 - val_AUC: 0.8063 - val_loss: 0.5534 - learning_rate: 2.5000e-04\n",
      "Epoch 90/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8306 - loss: 0.5145 - val_AUC: 0.8059 - val_loss: 0.5517 - learning_rate: 2.5000e-04\n",
      "Epoch 91/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8319 - loss: 0.5130 - val_AUC: 0.8064 - val_loss: 0.5526 - learning_rate: 2.5000e-04\n",
      "Epoch 92/200\n",
      "\u001b[1m348/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8333 - loss: 0.5123\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8333 - loss: 0.5123 - val_AUC: 0.8064 - val_loss: 0.5526 - learning_rate: 2.5000e-04\n",
      "Epoch 93/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8338 - loss: 0.5108 - val_AUC: 0.8069 - val_loss: 0.5498 - learning_rate: 1.2500e-04\n",
      "Epoch 94/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8367 - loss: 0.5069 - val_AUC: 0.8065 - val_loss: 0.5517 - learning_rate: 1.2500e-04\n",
      "Epoch 95/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8362 - loss: 0.5074 - val_AUC: 0.8065 - val_loss: 0.5527 - learning_rate: 1.2500e-04\n",
      "Epoch 96/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8344 - loss: 0.5087 - val_AUC: 0.8066 - val_loss: 0.5518 - learning_rate: 1.2500e-04\n",
      "Epoch 97/200\n",
      "\u001b[1m346/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8356 - loss: 0.5074\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8356 - loss: 0.5074 - val_AUC: 0.8060 - val_loss: 0.5527 - learning_rate: 1.2500e-04\n",
      "Epoch 98/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8353 - loss: 0.5093 - val_AUC: 0.8061 - val_loss: 0.5519 - learning_rate: 6.2500e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8370 - loss: 0.5058 - val_AUC: 0.8065 - val_loss: 0.5517 - learning_rate: 6.2500e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - AUC: 0.8359 - loss: 0.5070 - val_AUC: 0.8065 - val_loss: 0.5510 - learning_rate: 6.2500e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8363 - loss: 0.5072 - val_AUC: 0.8065 - val_loss: 0.5509 - learning_rate: 6.2500e-05\n",
      "Epoch 102/200\n",
      "\u001b[1m346/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8360 - loss: 0.5062\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8360 - loss: 0.5063 - val_AUC: 0.8064 - val_loss: 0.5513 - learning_rate: 6.2500e-05\n",
      "Epoch 103/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8375 - loss: 0.5052 - val_AUC: 0.8063 - val_loss: 0.5508 - learning_rate: 3.1250e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 14/20 | HP Config 4 | Seed 100\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - AUC: 0.5531 - loss: 0.7400 - val_AUC: 0.7030 - val_loss: 0.6385 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.6622 - loss: 0.6644 - val_AUC: 0.7279 - val_loss: 0.6219 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.6927 - loss: 0.6445 - val_AUC: 0.7364 - val_loss: 0.6157 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7125 - loss: 0.6324 - val_AUC: 0.7464 - val_loss: 0.6064 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7305 - loss: 0.6187 - val_AUC: 0.7545 - val_loss: 0.5981 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7367 - loss: 0.6140 - val_AUC: 0.7587 - val_loss: 0.5935 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7459 - loss: 0.6068 - val_AUC: 0.7637 - val_loss: 0.5894 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7495 - loss: 0.6037 - val_AUC: 0.7673 - val_loss: 0.5867 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7534 - loss: 0.6002 - val_AUC: 0.7691 - val_loss: 0.5833 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7605 - loss: 0.5942 - val_AUC: 0.7712 - val_loss: 0.5815 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7620 - loss: 0.5928 - val_AUC: 0.7715 - val_loss: 0.5828 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7641 - loss: 0.5910 - val_AUC: 0.7776 - val_loss: 0.5751 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7682 - loss: 0.5864 - val_AUC: 0.7778 - val_loss: 0.5734 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7714 - loss: 0.5837 - val_AUC: 0.7797 - val_loss: 0.5724 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7712 - loss: 0.5833 - val_AUC: 0.7809 - val_loss: 0.5709 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7742 - loss: 0.5805 - val_AUC: 0.7798 - val_loss: 0.5725 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7743 - loss: 0.5800 - val_AUC: 0.7803 - val_loss: 0.5716 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7751 - loss: 0.5792 - val_AUC: 0.7832 - val_loss: 0.5674 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7782 - loss: 0.5763 - val_AUC: 0.7844 - val_loss: 0.5674 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7819 - loss: 0.5721 - val_AUC: 0.7838 - val_loss: 0.5669 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7808 - loss: 0.5728 - val_AUC: 0.7850 - val_loss: 0.5654 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7831 - loss: 0.5702 - val_AUC: 0.7844 - val_loss: 0.5657 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7848 - loss: 0.5684 - val_AUC: 0.7851 - val_loss: 0.5663 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7866 - loss: 0.5664 - val_AUC: 0.7881 - val_loss: 0.5616 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7859 - loss: 0.5672 - val_AUC: 0.7872 - val_loss: 0.5632 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7877 - loss: 0.5653 - val_AUC: 0.7886 - val_loss: 0.5606 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7895 - loss: 0.5634 - val_AUC: 0.7905 - val_loss: 0.5600 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7897 - loss: 0.5626 - val_AUC: 0.7891 - val_loss: 0.5601 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7897 - loss: 0.5617 - val_AUC: 0.7902 - val_loss: 0.5616 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7922 - loss: 0.5603 - val_AUC: 0.7898 - val_loss: 0.5600 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7917 - loss: 0.5606 - val_AUC: 0.7889 - val_loss: 0.5609 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m347/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.7911 - loss: 0.5613\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7912 - loss: 0.5612 - val_AUC: 0.7894 - val_loss: 0.5620 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7982 - loss: 0.5531 - val_AUC: 0.7963 - val_loss: 0.5532 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7980 - loss: 0.5540 - val_AUC: 0.7958 - val_loss: 0.5535 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7964 - loss: 0.5550 - val_AUC: 0.7956 - val_loss: 0.5533 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8007 - loss: 0.5510 - val_AUC: 0.7959 - val_loss: 0.5535 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8006 - loss: 0.5510 - val_AUC: 0.7959 - val_loss: 0.5539 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m345/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8019 - loss: 0.5486\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8019 - loss: 0.5486 - val_AUC: 0.7957 - val_loss: 0.5541 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8044 - loss: 0.5464 - val_AUC: 0.7972 - val_loss: 0.5526 - learning_rate: 2.5000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8058 - loss: 0.5443 - val_AUC: 0.7978 - val_loss: 0.5515 - learning_rate: 2.5000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8069 - loss: 0.5429 - val_AUC: 0.7981 - val_loss: 0.5514 - learning_rate: 2.5000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8070 - loss: 0.5424 - val_AUC: 0.7975 - val_loss: 0.5516 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8071 - loss: 0.5417 - val_AUC: 0.7979 - val_loss: 0.5510 - learning_rate: 2.5000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8073 - loss: 0.5415 - val_AUC: 0.7995 - val_loss: 0.5490 - learning_rate: 2.5000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8080 - loss: 0.5410 - val_AUC: 0.7990 - val_loss: 0.5494 - learning_rate: 2.5000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8100 - loss: 0.5387 - val_AUC: 0.7982 - val_loss: 0.5506 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8081 - loss: 0.5407 - val_AUC: 0.7979 - val_loss: 0.5499 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8082 - loss: 0.5413 - val_AUC: 0.7996 - val_loss: 0.5492 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8110 - loss: 0.5374\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8110 - loss: 0.5374 - val_AUC: 0.7995 - val_loss: 0.5490 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8109 - loss: 0.5374 - val_AUC: 0.8000 - val_loss: 0.5484 - learning_rate: 1.2500e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8107 - loss: 0.5378 - val_AUC: 0.7999 - val_loss: 0.5492 - learning_rate: 1.2500e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8123 - loss: 0.5371 - val_AUC: 0.8001 - val_loss: 0.5482 - learning_rate: 1.2500e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8106 - loss: 0.5382 - val_AUC: 0.8002 - val_loss: 0.5485 - learning_rate: 1.2500e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8124 - loss: 0.5356 - val_AUC: 0.8003 - val_loss: 0.5481 - learning_rate: 1.2500e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8113 - loss: 0.5367 - val_AUC: 0.8002 - val_loss: 0.5485 - learning_rate: 1.2500e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8094 - loss: 0.5394 - val_AUC: 0.8002 - val_loss: 0.5479 - learning_rate: 1.2500e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8114 - loss: 0.5376 - val_AUC: 0.8002 - val_loss: 0.5479 - learning_rate: 1.2500e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8099 - loss: 0.5389 - val_AUC: 0.8001 - val_loss: 0.5479 - learning_rate: 1.2500e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8128 - loss: 0.5348\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8128 - loss: 0.5348 - val_AUC: 0.8003 - val_loss: 0.5479 - learning_rate: 1.2500e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8142 - loss: 0.5332 - val_AUC: 0.8005 - val_loss: 0.5478 - learning_rate: 6.2500e-05\n",
      "Epoch 61/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8122 - loss: 0.5352 - val_AUC: 0.8002 - val_loss: 0.5482 - learning_rate: 6.2500e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8171 - loss: 0.5301 - val_AUC: 0.8003 - val_loss: 0.5478 - learning_rate: 6.2500e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8136 - loss: 0.5329 - val_AUC: 0.8006 - val_loss: 0.5478 - learning_rate: 6.2500e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8139 - loss: 0.5335 - val_AUC: 0.8007 - val_loss: 0.5474 - learning_rate: 6.2500e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8140 - loss: 0.5338 - val_AUC: 0.8005 - val_loss: 0.5472 - learning_rate: 6.2500e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8160 - loss: 0.5311 - val_AUC: 0.8008 - val_loss: 0.5472 - learning_rate: 6.2500e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8113 - loss: 0.5361 - val_AUC: 0.8009 - val_loss: 0.5471 - learning_rate: 6.2500e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8137 - loss: 0.5333 - val_AUC: 0.8009 - val_loss: 0.5471 - learning_rate: 6.2500e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8141 - loss: 0.5329 - val_AUC: 0.8012 - val_loss: 0.5467 - learning_rate: 6.2500e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8128 - loss: 0.5338 - val_AUC: 0.8012 - val_loss: 0.5465 - learning_rate: 6.2500e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8166 - loss: 0.5306 - val_AUC: 0.8013 - val_loss: 0.5465 - learning_rate: 6.2500e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8152 - loss: 0.5318 - val_AUC: 0.8013 - val_loss: 0.5467 - learning_rate: 6.2500e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8146 - loss: 0.5327 - val_AUC: 0.8014 - val_loss: 0.5465 - learning_rate: 6.2500e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8165 - loss: 0.5307 - val_AUC: 0.8012 - val_loss: 0.5465 - learning_rate: 6.2500e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8158 - loss: 0.5303 - val_AUC: 0.8014 - val_loss: 0.5469 - learning_rate: 6.2500e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8154 - loss: 0.5313 - val_AUC: 0.8009 - val_loss: 0.5475 - learning_rate: 6.2500e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8166 - loss: 0.5300\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8166 - loss: 0.5300 - val_AUC: 0.8013 - val_loss: 0.5465 - learning_rate: 6.2500e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8151 - loss: 0.5319 - val_AUC: 0.8015 - val_loss: 0.5462 - learning_rate: 3.1250e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8154 - loss: 0.5309 - val_AUC: 0.8014 - val_loss: 0.5461 - learning_rate: 3.1250e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8146 - loss: 0.5318 - val_AUC: 0.8014 - val_loss: 0.5462 - learning_rate: 3.1250e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8150 - loss: 0.5316 - val_AUC: 0.8014 - val_loss: 0.5462 - learning_rate: 3.1250e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8146 - loss: 0.5318 - val_AUC: 0.8014 - val_loss: 0.5461 - learning_rate: 3.1250e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m349/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - AUC: 0.8166 - loss: 0.5303\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8166 - loss: 0.5303 - val_AUC: 0.8013 - val_loss: 0.5462 - learning_rate: 3.1250e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8140 - loss: 0.5330 - val_AUC: 0.8013 - val_loss: 0.5461 - learning_rate: 1.5625e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8165 - loss: 0.5299 - val_AUC: 0.8012 - val_loss: 0.5463 - learning_rate: 1.5625e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8152 - loss: 0.5316 - val_AUC: 0.8014 - val_loss: 0.5460 - learning_rate: 1.5625e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8164 - loss: 0.5296 - val_AUC: 0.8014 - val_loss: 0.5464 - learning_rate: 1.5625e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m347/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8163 - loss: 0.5300\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8163 - loss: 0.5300 - val_AUC: 0.8015 - val_loss: 0.5459 - learning_rate: 1.5625e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8135 - loss: 0.5333 - val_AUC: 0.8014 - val_loss: 0.5459 - learning_rate: 7.8125e-06\n",
      "Epoch 90/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8142 - loss: 0.5320 - val_AUC: 0.8014 - val_loss: 0.5459 - learning_rate: 7.8125e-06\n",
      "Epoch 91/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8154 - loss: 0.5313 - val_AUC: 0.8015 - val_loss: 0.5460 - learning_rate: 7.8125e-06\n",
      "Epoch 92/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8164 - loss: 0.5302 - val_AUC: 0.8015 - val_loss: 0.5461 - learning_rate: 7.8125e-06\n",
      "Epoch 93/200\n",
      "\u001b[1m345/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8182 - loss: 0.5278\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8182 - loss: 0.5278 - val_AUC: 0.8014 - val_loss: 0.5462 - learning_rate: 7.8125e-06\n",
      "Epoch 94/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8167 - loss: 0.5295 - val_AUC: 0.8014 - val_loss: 0.5460 - learning_rate: 3.9063e-06\n",
      "Epoch 95/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8186 - loss: 0.5265 - val_AUC: 0.8014 - val_loss: 0.5463 - learning_rate: 3.9063e-06\n",
      "Epoch 96/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8177 - loss: 0.5289 - val_AUC: 0.8015 - val_loss: 0.5461 - learning_rate: 3.9063e-06\n",
      "Epoch 97/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8153 - loss: 0.5300 - val_AUC: 0.8014 - val_loss: 0.5459 - learning_rate: 3.9063e-06\n",
      "Epoch 98/200\n",
      "\u001b[1m349/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8175 - loss: 0.5285\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8175 - loss: 0.5285 - val_AUC: 0.8014 - val_loss: 0.5460 - learning_rate: 3.9063e-06\n",
      "Epoch 99/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8179 - loss: 0.5289 - val_AUC: 0.8015 - val_loss: 0.5459 - learning_rate: 1.9531e-06\n",
      "Epoch 100/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8149 - loss: 0.5325 - val_AUC: 0.8015 - val_loss: 0.5460 - learning_rate: 1.9531e-06\n",
      "Epoch 101/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8179 - loss: 0.5282 - val_AUC: 0.8014 - val_loss: 0.5460 - learning_rate: 1.9531e-06\n",
      "Epoch 102/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8165 - loss: 0.5294 - val_AUC: 0.8014 - val_loss: 0.5460 - learning_rate: 1.9531e-06\n",
      "Epoch 103/200\n",
      "\u001b[1m346/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8177 - loss: 0.5286\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8178 - loss: 0.5285 - val_AUC: 0.8015 - val_loss: 0.5461 - learning_rate: 1.9531e-06\n",
      "Epoch 104/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8169 - loss: 0.5297 - val_AUC: 0.8014 - val_loss: 0.5460 - learning_rate: 9.7656e-07\n",
      "Epoch 105/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8173 - loss: 0.5285 - val_AUC: 0.8014 - val_loss: 0.5461 - learning_rate: 9.7656e-07\n",
      "Epoch 106/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8186 - loss: 0.5271 - val_AUC: 0.8015 - val_loss: 0.5460 - learning_rate: 9.7656e-07\n",
      "Epoch 107/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8155 - loss: 0.5311 - val_AUC: 0.8015 - val_loss: 0.5460 - learning_rate: 9.7656e-07\n",
      "Epoch 108/200\n",
      "\u001b[1m345/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8151 - loss: 0.5317\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8151 - loss: 0.5316 - val_AUC: 0.8015 - val_loss: 0.5459 - learning_rate: 9.7656e-07\n",
      "Epoch 109/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8159 - loss: 0.5306 - val_AUC: 0.8015 - val_loss: 0.5461 - learning_rate: 4.8828e-07\n",
      "Epoch 110/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8168 - loss: 0.5293 - val_AUC: 0.8015 - val_loss: 0.5458 - learning_rate: 4.8828e-07\n",
      "Epoch 111/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8173 - loss: 0.5295 - val_AUC: 0.8015 - val_loss: 0.5460 - learning_rate: 4.8828e-07\n",
      "Epoch 112/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8167 - loss: 0.5298 - val_AUC: 0.8015 - val_loss: 0.5458 - learning_rate: 4.8828e-07\n",
      "Epoch 113/200\n",
      "\u001b[1m349/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8177 - loss: 0.5282\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8177 - loss: 0.5282 - val_AUC: 0.8015 - val_loss: 0.5460 - learning_rate: 4.8828e-07\n",
      "Epoch 114/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8173 - loss: 0.5287 - val_AUC: 0.8015 - val_loss: 0.5457 - learning_rate: 2.4414e-07\n",
      "Epoch 115/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8177 - loss: 0.5283 - val_AUC: 0.8015 - val_loss: 0.5459 - learning_rate: 2.4414e-07\n",
      "Epoch 116/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8169 - loss: 0.5286 - val_AUC: 0.8015 - val_loss: 0.5459 - learning_rate: 2.4414e-07\n",
      "Epoch 117/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8175 - loss: 0.5280 - val_AUC: 0.8015 - val_loss: 0.5459 - learning_rate: 2.4414e-07\n",
      "Epoch 118/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8161 - loss: 0.5293\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8161 - loss: 0.5293 - val_AUC: 0.8015 - val_loss: 0.5460 - learning_rate: 2.4414e-07\n",
      "Epoch 119/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8134 - loss: 0.5331 - val_AUC: 0.8014 - val_loss: 0.5459 - learning_rate: 1.2207e-07\n",
      "Epoch 120/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.8166 - loss: 0.5298 - val_AUC: 0.8014 - val_loss: 0.5460 - learning_rate: 1.2207e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 15/20 | HP Config 4 | Seed 200\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - AUC: 0.5344 - loss: 0.7577 - val_AUC: 0.6994 - val_loss: 0.6509 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.6515 - loss: 0.6710 - val_AUC: 0.7232 - val_loss: 0.6279 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.6908 - loss: 0.6479 - val_AUC: 0.7347 - val_loss: 0.6153 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7078 - loss: 0.6364 - val_AUC: 0.7440 - val_loss: 0.6075 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7220 - loss: 0.6266 - val_AUC: 0.7533 - val_loss: 0.5988 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7346 - loss: 0.6164 - val_AUC: 0.7603 - val_loss: 0.5925 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7421 - loss: 0.6097 - val_AUC: 0.7630 - val_loss: 0.5882 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7480 - loss: 0.6047 - val_AUC: 0.7663 - val_loss: 0.5862 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7520 - loss: 0.6020 - val_AUC: 0.7703 - val_loss: 0.5821 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7543 - loss: 0.5999 - val_AUC: 0.7738 - val_loss: 0.5785 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7601 - loss: 0.5940 - val_AUC: 0.7751 - val_loss: 0.5787 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7598 - loss: 0.5947 - val_AUC: 0.7775 - val_loss: 0.5735 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7647 - loss: 0.5892 - val_AUC: 0.7784 - val_loss: 0.5740 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7646 - loss: 0.5889 - val_AUC: 0.7835 - val_loss: 0.5698 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7704 - loss: 0.5836 - val_AUC: 0.7844 - val_loss: 0.5667 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7724 - loss: 0.5805 - val_AUC: 0.7851 - val_loss: 0.5657 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7752 - loss: 0.5787 - val_AUC: 0.7877 - val_loss: 0.5632 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7752 - loss: 0.5786 - val_AUC: 0.7858 - val_loss: 0.5654 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7762 - loss: 0.5771 - val_AUC: 0.7877 - val_loss: 0.5636 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7779 - loss: 0.5760 - val_AUC: 0.7881 - val_loss: 0.5633 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7828 - loss: 0.5702 - val_AUC: 0.7889 - val_loss: 0.5628 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7802 - loss: 0.5734 - val_AUC: 0.7909 - val_loss: 0.5600 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7832 - loss: 0.5701 - val_AUC: 0.7912 - val_loss: 0.5600 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7838 - loss: 0.5696 - val_AUC: 0.7917 - val_loss: 0.5590 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7840 - loss: 0.5693 - val_AUC: 0.7911 - val_loss: 0.5596 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7861 - loss: 0.5671 - val_AUC: 0.7927 - val_loss: 0.5579 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7858 - loss: 0.5670 - val_AUC: 0.7926 - val_loss: 0.5578 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7892 - loss: 0.5642 - val_AUC: 0.7938 - val_loss: 0.5567 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7874 - loss: 0.5658 - val_AUC: 0.7947 - val_loss: 0.5567 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7880 - loss: 0.5651 - val_AUC: 0.7946 - val_loss: 0.5551 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7898 - loss: 0.5630 - val_AUC: 0.7951 - val_loss: 0.5557 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7916 - loss: 0.5612 - val_AUC: 0.7943 - val_loss: 0.5555 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7922 - loss: 0.5603 - val_AUC: 0.7969 - val_loss: 0.5530 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7947 - loss: 0.5580 - val_AUC: 0.7974 - val_loss: 0.5528 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7929 - loss: 0.5603 - val_AUC: 0.7977 - val_loss: 0.5519 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7922 - loss: 0.5605 - val_AUC: 0.7963 - val_loss: 0.5534 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7942 - loss: 0.5587 - val_AUC: 0.7972 - val_loss: 0.5537 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7962 - loss: 0.5562 - val_AUC: 0.7983 - val_loss: 0.5519 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7930 - loss: 0.5592 - val_AUC: 0.7976 - val_loss: 0.5537 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7964 - loss: 0.5558 - val_AUC: 0.7989 - val_loss: 0.5520 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7985 - loss: 0.5536 - val_AUC: 0.7992 - val_loss: 0.5522 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7989 - loss: 0.5540 - val_AUC: 0.7991 - val_loss: 0.5518 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7986 - loss: 0.5549 - val_AUC: 0.7995 - val_loss: 0.5512 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8004 - loss: 0.5514 - val_AUC: 0.7992 - val_loss: 0.5514 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8000 - loss: 0.5534 - val_AUC: 0.7994 - val_loss: 0.5520 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8022 - loss: 0.5504 - val_AUC: 0.7997 - val_loss: 0.5512 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8017 - loss: 0.5510 - val_AUC: 0.8003 - val_loss: 0.5507 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8021 - loss: 0.5505 - val_AUC: 0.7995 - val_loss: 0.5522 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8031 - loss: 0.5495 - val_AUC: 0.7998 - val_loss: 0.5520 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8034 - loss: 0.5490 - val_AUC: 0.7993 - val_loss: 0.5515 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8062 - loss: 0.5465 - val_AUC: 0.8012 - val_loss: 0.5512 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8040 - loss: 0.5487 - val_AUC: 0.8006 - val_loss: 0.5520 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8060 - loss: 0.5466 - val_AUC: 0.8016 - val_loss: 0.5502 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8062 - loss: 0.5465 - val_AUC: 0.8014 - val_loss: 0.5498 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8098 - loss: 0.5419 - val_AUC: 0.8018 - val_loss: 0.5509 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8065 - loss: 0.5466 - val_AUC: 0.7998 - val_loss: 0.5522 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8090 - loss: 0.5436 - val_AUC: 0.7994 - val_loss: 0.5523 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8078 - loss: 0.5444 - val_AUC: 0.7995 - val_loss: 0.5530 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8088 - loss: 0.5431 - val_AUC: 0.7997 - val_loss: 0.5528 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m348/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8111 - loss: 0.5411\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8112 - loss: 0.5411 - val_AUC: 0.8005 - val_loss: 0.5526 - learning_rate: 0.0010\n",
      "Epoch 61/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8143 - loss: 0.5375 - val_AUC: 0.8013 - val_loss: 0.5516 - learning_rate: 5.0000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8159 - loss: 0.5349 - val_AUC: 0.8023 - val_loss: 0.5504 - learning_rate: 5.0000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8131 - loss: 0.5384 - val_AUC: 0.8028 - val_loss: 0.5503 - learning_rate: 5.0000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8184 - loss: 0.5325 - val_AUC: 0.8024 - val_loss: 0.5495 - learning_rate: 5.0000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8188 - loss: 0.5310 - val_AUC: 0.8020 - val_loss: 0.5506 - learning_rate: 5.0000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8188 - loss: 0.5306 - val_AUC: 0.8025 - val_loss: 0.5500 - learning_rate: 5.0000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8174 - loss: 0.5322 - val_AUC: 0.8019 - val_loss: 0.5501 - learning_rate: 5.0000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m346/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8173 - loss: 0.5337\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8174 - loss: 0.5335 - val_AUC: 0.8020 - val_loss: 0.5513 - learning_rate: 5.0000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8233 - loss: 0.5253 - val_AUC: 0.8023 - val_loss: 0.5512 - learning_rate: 2.5000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8235 - loss: 0.5256 - val_AUC: 0.8028 - val_loss: 0.5505 - learning_rate: 2.5000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8244 - loss: 0.5234 - val_AUC: 0.8025 - val_loss: 0.5508 - learning_rate: 2.5000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8245 - loss: 0.5232 - val_AUC: 0.8014 - val_loss: 0.5514 - learning_rate: 2.5000e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m349/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8234 - loss: 0.5246\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8235 - loss: 0.5245 - val_AUC: 0.8019 - val_loss: 0.5510 - learning_rate: 2.5000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 16/20 | HP Config 4 | Seed 300\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - AUC: 0.5443 - loss: 0.7455 - val_AUC: 0.6973 - val_loss: 0.6482 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.6675 - loss: 0.6609 - val_AUC: 0.7227 - val_loss: 0.6275 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7021 - loss: 0.6399 - val_AUC: 0.7364 - val_loss: 0.6163 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.7189 - loss: 0.6285 - val_AUC: 0.7436 - val_loss: 0.6092 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7300 - loss: 0.6202 - val_AUC: 0.7521 - val_loss: 0.6003 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7391 - loss: 0.6130 - val_AUC: 0.7567 - val_loss: 0.5949 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7498 - loss: 0.6034 - val_AUC: 0.7622 - val_loss: 0.5906 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7511 - loss: 0.6025 - val_AUC: 0.7647 - val_loss: 0.5866 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7577 - loss: 0.5960 - val_AUC: 0.7703 - val_loss: 0.5820 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7605 - loss: 0.5944 - val_AUC: 0.7721 - val_loss: 0.5801 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7643 - loss: 0.5899 - val_AUC: 0.7731 - val_loss: 0.5790 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7695 - loss: 0.5855 - val_AUC: 0.7767 - val_loss: 0.5753 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7708 - loss: 0.5841 - val_AUC: 0.7786 - val_loss: 0.5730 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7723 - loss: 0.5822 - val_AUC: 0.7802 - val_loss: 0.5720 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7733 - loss: 0.5816 - val_AUC: 0.7804 - val_loss: 0.5708 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7753 - loss: 0.5798 - val_AUC: 0.7827 - val_loss: 0.5686 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7798 - loss: 0.5741 - val_AUC: 0.7815 - val_loss: 0.5696 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7815 - loss: 0.5722 - val_AUC: 0.7846 - val_loss: 0.5663 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7821 - loss: 0.5724 - val_AUC: 0.7823 - val_loss: 0.5685 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7826 - loss: 0.5712 - val_AUC: 0.7850 - val_loss: 0.5649 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7845 - loss: 0.5695 - val_AUC: 0.7861 - val_loss: 0.5652 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7873 - loss: 0.5663 - val_AUC: 0.7869 - val_loss: 0.5645 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7858 - loss: 0.5677 - val_AUC: 0.7885 - val_loss: 0.5628 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7894 - loss: 0.5635 - val_AUC: 0.7890 - val_loss: 0.5622 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7906 - loss: 0.5636 - val_AUC: 0.7891 - val_loss: 0.5612 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7890 - loss: 0.5641 - val_AUC: 0.7908 - val_loss: 0.5600 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7919 - loss: 0.5605 - val_AUC: 0.7926 - val_loss: 0.5596 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7921 - loss: 0.5606 - val_AUC: 0.7900 - val_loss: 0.5605 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.7940 - loss: 0.5596 - val_AUC: 0.7923 - val_loss: 0.5580 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7957 - loss: 0.5570 - val_AUC: 0.7911 - val_loss: 0.5588 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7954 - loss: 0.5578 - val_AUC: 0.7933 - val_loss: 0.5573 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7973 - loss: 0.5552 - val_AUC: 0.7914 - val_loss: 0.5592 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7971 - loss: 0.5555 - val_AUC: 0.7938 - val_loss: 0.5547 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.7980 - loss: 0.5552 - val_AUC: 0.7930 - val_loss: 0.5563 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.7976 - loss: 0.5546 - val_AUC: 0.7942 - val_loss: 0.5562 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.8000 - loss: 0.5527 - val_AUC: 0.7949 - val_loss: 0.5564 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8002 - loss: 0.5522 - val_AUC: 0.7949 - val_loss: 0.5548 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8015 - loss: 0.5509 - val_AUC: 0.7944 - val_loss: 0.5582 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8014 - loss: 0.5522 - val_AUC: 0.7939 - val_loss: 0.5560 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8026 - loss: 0.5498 - val_AUC: 0.7946 - val_loss: 0.5557 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8028 - loss: 0.5501\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.8028 - loss: 0.5501 - val_AUC: 0.7943 - val_loss: 0.5551 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8068 - loss: 0.5445 - val_AUC: 0.7969 - val_loss: 0.5536 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8103 - loss: 0.5405 - val_AUC: 0.7968 - val_loss: 0.5524 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8079 - loss: 0.5437 - val_AUC: 0.7972 - val_loss: 0.5527 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8091 - loss: 0.5419 - val_AUC: 0.7993 - val_loss: 0.5510 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8109 - loss: 0.5392 - val_AUC: 0.7992 - val_loss: 0.5508 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.8111 - loss: 0.5397 - val_AUC: 0.7982 - val_loss: 0.5516 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.8107 - loss: 0.5400 - val_AUC: 0.7993 - val_loss: 0.5498 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.8112 - loss: 0.5388 - val_AUC: 0.7988 - val_loss: 0.5510 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8115 - loss: 0.5379\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8115 - loss: 0.5379 - val_AUC: 0.7988 - val_loss: 0.5504 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.8141 - loss: 0.5349 - val_AUC: 0.7995 - val_loss: 0.5504 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8157 - loss: 0.5330 - val_AUC: 0.8000 - val_loss: 0.5501 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.8153 - loss: 0.5337 - val_AUC: 0.8002 - val_loss: 0.5490 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.8170 - loss: 0.5318 - val_AUC: 0.8009 - val_loss: 0.5497 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8163 - loss: 0.5323 - val_AUC: 0.8011 - val_loss: 0.5491 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8172 - loss: 0.5312 - val_AUC: 0.8010 - val_loss: 0.5489 - learning_rate: 2.5000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8178 - loss: 0.5312 - val_AUC: 0.8007 - val_loss: 0.5488 - learning_rate: 2.5000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8167 - loss: 0.5309 - val_AUC: 0.8008 - val_loss: 0.5484 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8160 - loss: 0.5320 - val_AUC: 0.8012 - val_loss: 0.5481 - learning_rate: 2.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8196 - loss: 0.5282 - val_AUC: 0.8013 - val_loss: 0.5488 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8195 - loss: 0.5283 - val_AUC: 0.7999 - val_loss: 0.5498 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8184 - loss: 0.5292 - val_AUC: 0.8010 - val_loss: 0.5491 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8199 - loss: 0.5275 - val_AUC: 0.8005 - val_loss: 0.5498 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8180 - loss: 0.5300\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8180 - loss: 0.5300 - val_AUC: 0.8005 - val_loss: 0.5482 - learning_rate: 2.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8220 - loss: 0.5250 - val_AUC: 0.8009 - val_loss: 0.5498 - learning_rate: 1.2500e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8211 - loss: 0.5254 - val_AUC: 0.8007 - val_loss: 0.5494 - learning_rate: 1.2500e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8218 - loss: 0.5250 - val_AUC: 0.8007 - val_loss: 0.5498 - learning_rate: 1.2500e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8199 - loss: 0.5267 - val_AUC: 0.8011 - val_loss: 0.5496 - learning_rate: 1.2500e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8215 - loss: 0.5252 - val_AUC: 0.8018 - val_loss: 0.5485 - learning_rate: 1.2500e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8228 - loss: 0.5237 - val_AUC: 0.8017 - val_loss: 0.5487 - learning_rate: 1.2500e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8236 - loss: 0.5223 - val_AUC: 0.8017 - val_loss: 0.5489 - learning_rate: 1.2500e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8204 - loss: 0.5257 - val_AUC: 0.8013 - val_loss: 0.5488 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8231 - loss: 0.5229 - val_AUC: 0.8015 - val_loss: 0.5481 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8246 - loss: 0.5213 - val_AUC: 0.8019 - val_loss: 0.5484 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8237 - loss: 0.5233 - val_AUC: 0.8015 - val_loss: 0.5491 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8227 - loss: 0.5232 - val_AUC: 0.8015 - val_loss: 0.5497 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8214 - loss: 0.5244 - val_AUC: 0.8018 - val_loss: 0.5485 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8250 - loss: 0.5206 - val_AUC: 0.8016 - val_loss: 0.5485 - learning_rate: 1.2500e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m349/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8247 - loss: 0.5210\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8247 - loss: 0.5210 - val_AUC: 0.8019 - val_loss: 0.5487 - learning_rate: 1.2500e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8241 - loss: 0.5220 - val_AUC: 0.8018 - val_loss: 0.5491 - learning_rate: 6.2500e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8273 - loss: 0.5178 - val_AUC: 0.8019 - val_loss: 0.5495 - learning_rate: 6.2500e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8235 - loss: 0.5224 - val_AUC: 0.8020 - val_loss: 0.5480 - learning_rate: 6.2500e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8273 - loss: 0.5176 - val_AUC: 0.8020 - val_loss: 0.5490 - learning_rate: 6.2500e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8247 - loss: 0.5209 - val_AUC: 0.8020 - val_loss: 0.5486 - learning_rate: 6.2500e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.8248 - loss: 0.5202 - val_AUC: 0.8026 - val_loss: 0.5478 - learning_rate: 6.2500e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8260 - loss: 0.5186 - val_AUC: 0.8023 - val_loss: 0.5479 - learning_rate: 6.2500e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8237 - loss: 0.5214 - val_AUC: 0.8022 - val_loss: 0.5476 - learning_rate: 6.2500e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8248 - loss: 0.5203 - val_AUC: 0.8023 - val_loss: 0.5479 - learning_rate: 6.2500e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8263 - loss: 0.5185 - val_AUC: 0.8021 - val_loss: 0.5483 - learning_rate: 6.2500e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m347/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8267 - loss: 0.5176\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8267 - loss: 0.5176 - val_AUC: 0.8019 - val_loss: 0.5489 - learning_rate: 6.2500e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8237 - loss: 0.5222 - val_AUC: 0.8021 - val_loss: 0.5485 - learning_rate: 3.1250e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8258 - loss: 0.5195 - val_AUC: 0.8018 - val_loss: 0.5491 - learning_rate: 3.1250e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8228 - loss: 0.5229 - val_AUC: 0.8019 - val_loss: 0.5485 - learning_rate: 3.1250e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8271 - loss: 0.5185 - val_AUC: 0.8019 - val_loss: 0.5482 - learning_rate: 3.1250e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.8236 - loss: 0.5219\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.8236 - loss: 0.5218 - val_AUC: 0.8020 - val_loss: 0.5488 - learning_rate: 3.1250e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 17/20 | HP Config 5 | Seed 0\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - AUC: 0.5936 - loss: 0.7222 - val_AUC: 0.6446 - val_loss: 0.6721 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6624 - loss: 0.6627 - val_AUC: 0.6748 - val_loss: 0.6566 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6953 - loss: 0.6415 - val_AUC: 0.7095 - val_loss: 0.6337 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7168 - loss: 0.6265 - val_AUC: 0.7341 - val_loss: 0.6131 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7358 - loss: 0.6126 - val_AUC: 0.7497 - val_loss: 0.6013 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7477 - loss: 0.6032 - val_AUC: 0.7557 - val_loss: 0.5956 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7543 - loss: 0.5974 - val_AUC: 0.7529 - val_loss: 0.5977 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7632 - loss: 0.5898 - val_AUC: 0.7560 - val_loss: 0.5938 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7651 - loss: 0.5875 - val_AUC: 0.7622 - val_loss: 0.5891 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7694 - loss: 0.5838 - val_AUC: 0.7634 - val_loss: 0.5879 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.7736 - loss: 0.5795 - val_AUC: 0.7684 - val_loss: 0.5832 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7768 - loss: 0.5765 - val_AUC: 0.7634 - val_loss: 0.5894 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7799 - loss: 0.5738 - val_AUC: 0.7668 - val_loss: 0.5856 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7826 - loss: 0.5707 - val_AUC: 0.7727 - val_loss: 0.5790 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7849 - loss: 0.5687 - val_AUC: 0.7734 - val_loss: 0.5797 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7875 - loss: 0.5659 - val_AUC: 0.7728 - val_loss: 0.5803 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7897 - loss: 0.5632 - val_AUC: 0.7725 - val_loss: 0.5823 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7924 - loss: 0.5609 - val_AUC: 0.7764 - val_loss: 0.5777 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7957 - loss: 0.5578 - val_AUC: 0.7789 - val_loss: 0.5750 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7993 - loss: 0.5534 - val_AUC: 0.7743 - val_loss: 0.5798 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7989 - loss: 0.5541 - val_AUC: 0.7795 - val_loss: 0.5750 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8015 - loss: 0.5512 - val_AUC: 0.7798 - val_loss: 0.5747 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8025 - loss: 0.5504 - val_AUC: 0.7800 - val_loss: 0.5763 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8051 - loss: 0.5473 - val_AUC: 0.7792 - val_loss: 0.5757 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8090 - loss: 0.5429 - val_AUC: 0.7819 - val_loss: 0.5745 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8095 - loss: 0.5433 - val_AUC: 0.7840 - val_loss: 0.5718 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8072 - loss: 0.5450 - val_AUC: 0.7843 - val_loss: 0.5724 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8113 - loss: 0.5407 - val_AUC: 0.7856 - val_loss: 0.5714 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8140 - loss: 0.5374 - val_AUC: 0.7868 - val_loss: 0.5729 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8153 - loss: 0.5362 - val_AUC: 0.7828 - val_loss: 0.5762 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8172 - loss: 0.5336 - val_AUC: 0.7859 - val_loss: 0.5725 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8173 - loss: 0.5343 - val_AUC: 0.7853 - val_loss: 0.5739 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8192 - loss: 0.5317 - val_AUC: 0.7830 - val_loss: 0.5800 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m347/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8226 - loss: 0.5276\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8226 - loss: 0.5276 - val_AUC: 0.7832 - val_loss: 0.5803 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8291 - loss: 0.5200 - val_AUC: 0.7928 - val_loss: 0.5679 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8319 - loss: 0.5163 - val_AUC: 0.7915 - val_loss: 0.5703 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8329 - loss: 0.5147 - val_AUC: 0.7920 - val_loss: 0.5704 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8346 - loss: 0.5130 - val_AUC: 0.7918 - val_loss: 0.5714 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8345 - loss: 0.5128 - val_AUC: 0.7897 - val_loss: 0.5740 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8344 - loss: 0.5124\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8345 - loss: 0.5124 - val_AUC: 0.7898 - val_loss: 0.5742 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8393 - loss: 0.5062 - val_AUC: 0.7951 - val_loss: 0.5674 - learning_rate: 2.5000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8429 - loss: 0.5012 - val_AUC: 0.7947 - val_loss: 0.5695 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8437 - loss: 0.5005 - val_AUC: 0.7953 - val_loss: 0.5673 - learning_rate: 2.5000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8432 - loss: 0.5008 - val_AUC: 0.7963 - val_loss: 0.5691 - learning_rate: 2.5000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8441 - loss: 0.4994 - val_AUC: 0.7968 - val_loss: 0.5682 - learning_rate: 2.5000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8455 - loss: 0.4976 - val_AUC: 0.7958 - val_loss: 0.5686 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8459 - loss: 0.4970 - val_AUC: 0.7947 - val_loss: 0.5705 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8492 - loss: 0.4927 - val_AUC: 0.7947 - val_loss: 0.5712 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8451 - loss: 0.4980 - val_AUC: 0.7961 - val_loss: 0.5697 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8484 - loss: 0.4938\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8484 - loss: 0.4938 - val_AUC: 0.7957 - val_loss: 0.5700 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8505 - loss: 0.4904 - val_AUC: 0.7978 - val_loss: 0.5684 - learning_rate: 1.2500e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8524 - loss: 0.4879 - val_AUC: 0.7970 - val_loss: 0.5692 - learning_rate: 1.2500e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8520 - loss: 0.4884 - val_AUC: 0.7970 - val_loss: 0.5704 - learning_rate: 1.2500e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8524 - loss: 0.4875 - val_AUC: 0.7973 - val_loss: 0.5682 - learning_rate: 1.2500e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8530 - loss: 0.4866 - val_AUC: 0.7968 - val_loss: 0.5702 - learning_rate: 1.2500e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m342/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8514 - loss: 0.4886\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8514 - loss: 0.4885 - val_AUC: 0.7967 - val_loss: 0.5711 - learning_rate: 1.2500e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8526 - loss: 0.4877 - val_AUC: 0.7966 - val_loss: 0.5695 - learning_rate: 6.2500e-05\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8554 - loss: 0.4833 - val_AUC: 0.7968 - val_loss: 0.5692 - learning_rate: 6.2500e-05\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8537 - loss: 0.4856 - val_AUC: 0.7964 - val_loss: 0.5700 - learning_rate: 6.2500e-05\n",
      "Epoch 60/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8555 - loss: 0.4833 - val_AUC: 0.7977 - val_loss: 0.5687 - learning_rate: 6.2500e-05\n",
      "Epoch 61/200\n",
      "\u001b[1m348/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8559 - loss: 0.4834\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8559 - loss: 0.4834 - val_AUC: 0.7975 - val_loss: 0.5699 - learning_rate: 6.2500e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 18/20 | HP Config 5 | Seed 100\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - AUC: 0.5897 - loss: 0.7290 - val_AUC: 0.6343 - val_loss: 0.6948 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6539 - loss: 0.6681 - val_AUC: 0.6724 - val_loss: 0.6576 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6871 - loss: 0.6459 - val_AUC: 0.7061 - val_loss: 0.6353 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7034 - loss: 0.6353 - val_AUC: 0.7196 - val_loss: 0.6263 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7241 - loss: 0.6203 - val_AUC: 0.7448 - val_loss: 0.6051 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7398 - loss: 0.6088 - val_AUC: 0.7601 - val_loss: 0.5912 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7532 - loss: 0.5982 - val_AUC: 0.7615 - val_loss: 0.5897 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7588 - loss: 0.5932 - val_AUC: 0.7649 - val_loss: 0.5869 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7645 - loss: 0.5881 - val_AUC: 0.7680 - val_loss: 0.5840 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7664 - loss: 0.5863 - val_AUC: 0.7749 - val_loss: 0.5773 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7730 - loss: 0.5799 - val_AUC: 0.7745 - val_loss: 0.5781 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7738 - loss: 0.5792 - val_AUC: 0.7755 - val_loss: 0.5777 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7812 - loss: 0.5723 - val_AUC: 0.7762 - val_loss: 0.5761 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7825 - loss: 0.5711 - val_AUC: 0.7793 - val_loss: 0.5731 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7854 - loss: 0.5684 - val_AUC: 0.7796 - val_loss: 0.5731 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7892 - loss: 0.5642 - val_AUC: 0.7836 - val_loss: 0.5679 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7891 - loss: 0.5641 - val_AUC: 0.7857 - val_loss: 0.5665 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7935 - loss: 0.5599 - val_AUC: 0.7868 - val_loss: 0.5659 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7931 - loss: 0.5600 - val_AUC: 0.7832 - val_loss: 0.5700 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7964 - loss: 0.5568 - val_AUC: 0.7841 - val_loss: 0.5689 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7991 - loss: 0.5540 - val_AUC: 0.7891 - val_loss: 0.5634 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7995 - loss: 0.5537 - val_AUC: 0.7902 - val_loss: 0.5622 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8030 - loss: 0.5501 - val_AUC: 0.7906 - val_loss: 0.5626 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8036 - loss: 0.5496 - val_AUC: 0.7902 - val_loss: 0.5627 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8065 - loss: 0.5462 - val_AUC: 0.7894 - val_loss: 0.5646 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8084 - loss: 0.5441 - val_AUC: 0.7916 - val_loss: 0.5610 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.8093 - loss: 0.5431 - val_AUC: 0.7913 - val_loss: 0.5628 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8121 - loss: 0.5399 - val_AUC: 0.7901 - val_loss: 0.5650 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8142 - loss: 0.5377 - val_AUC: 0.7900 - val_loss: 0.5648 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8149 - loss: 0.5371 - val_AUC: 0.7911 - val_loss: 0.5632 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8165 - loss: 0.5353\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8165 - loss: 0.5353 - val_AUC: 0.7908 - val_loss: 0.5644 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8257 - loss: 0.5242 - val_AUC: 0.7964 - val_loss: 0.5596 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8266 - loss: 0.5234 - val_AUC: 0.7950 - val_loss: 0.5618 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8283 - loss: 0.5207 - val_AUC: 0.7963 - val_loss: 0.5615 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8313 - loss: 0.5171 - val_AUC: 0.7950 - val_loss: 0.5620 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8314 - loss: 0.5168 - val_AUC: 0.7945 - val_loss: 0.5642 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m342/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8326 - loss: 0.5154\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8326 - loss: 0.5154 - val_AUC: 0.7932 - val_loss: 0.5649 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8352 - loss: 0.5116 - val_AUC: 0.7973 - val_loss: 0.5616 - learning_rate: 2.5000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8395 - loss: 0.5064 - val_AUC: 0.7965 - val_loss: 0.5627 - learning_rate: 2.5000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8373 - loss: 0.5087 - val_AUC: 0.7965 - val_loss: 0.5637 - learning_rate: 2.5000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8377 - loss: 0.5079 - val_AUC: 0.7961 - val_loss: 0.5636 - learning_rate: 2.5000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8413 - loss: 0.5036 - val_AUC: 0.7961 - val_loss: 0.5639 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8408 - loss: 0.5041\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8408 - loss: 0.5041 - val_AUC: 0.7962 - val_loss: 0.5665 - learning_rate: 2.5000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8428 - loss: 0.5013 - val_AUC: 0.7968 - val_loss: 0.5638 - learning_rate: 1.2500e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8428 - loss: 0.5012 - val_AUC: 0.7980 - val_loss: 0.5629 - learning_rate: 1.2500e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8463 - loss: 0.4965 - val_AUC: 0.7973 - val_loss: 0.5660 - learning_rate: 1.2500e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8439 - loss: 0.4998 - val_AUC: 0.7971 - val_loss: 0.5647 - learning_rate: 1.2500e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8438 - loss: 0.4993 - val_AUC: 0.7969 - val_loss: 0.5644 - learning_rate: 1.2500e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8465 - loss: 0.4966 - val_AUC: 0.7971 - val_loss: 0.5653 - learning_rate: 1.2500e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8443 - loss: 0.4991\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8443 - loss: 0.4991 - val_AUC: 0.7964 - val_loss: 0.5658 - learning_rate: 1.2500e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8467 - loss: 0.4959 - val_AUC: 0.7969 - val_loss: 0.5654 - learning_rate: 6.2500e-05\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8484 - loss: 0.4936 - val_AUC: 0.7972 - val_loss: 0.5653 - learning_rate: 6.2500e-05\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8480 - loss: 0.4934 - val_AUC: 0.7970 - val_loss: 0.5657 - learning_rate: 6.2500e-05\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8482 - loss: 0.4941 - val_AUC: 0.7969 - val_loss: 0.5664 - learning_rate: 6.2500e-05\n",
      "Epoch 55/200\n",
      "\u001b[1m349/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8499 - loss: 0.4915\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8499 - loss: 0.4915 - val_AUC: 0.7969 - val_loss: 0.5659 - learning_rate: 6.2500e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 19/20 | HP Config 5 | Seed 200\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - AUC: 0.5914 - loss: 0.7239 - val_AUC: 0.6629 - val_loss: 0.6684 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6599 - loss: 0.6649 - val_AUC: 0.6925 - val_loss: 0.6490 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6950 - loss: 0.6423 - val_AUC: 0.7131 - val_loss: 0.6324 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7140 - loss: 0.6289 - val_AUC: 0.7356 - val_loss: 0.6170 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7359 - loss: 0.6128 - val_AUC: 0.7482 - val_loss: 0.6040 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7451 - loss: 0.6053 - val_AUC: 0.7606 - val_loss: 0.5918 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7540 - loss: 0.5979 - val_AUC: 0.7608 - val_loss: 0.5934 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7628 - loss: 0.5903 - val_AUC: 0.7630 - val_loss: 0.5906 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7644 - loss: 0.5886 - val_AUC: 0.7691 - val_loss: 0.5839 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7691 - loss: 0.5844 - val_AUC: 0.7667 - val_loss: 0.5879 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7713 - loss: 0.5815 - val_AUC: 0.7723 - val_loss: 0.5813 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7750 - loss: 0.5785 - val_AUC: 0.7730 - val_loss: 0.5815 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7785 - loss: 0.5754 - val_AUC: 0.7737 - val_loss: 0.5808 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7795 - loss: 0.5738 - val_AUC: 0.7699 - val_loss: 0.5870 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7824 - loss: 0.5716 - val_AUC: 0.7730 - val_loss: 0.5835 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7844 - loss: 0.5690 - val_AUC: 0.7783 - val_loss: 0.5765 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7870 - loss: 0.5664 - val_AUC: 0.7755 - val_loss: 0.5842 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7896 - loss: 0.5642 - val_AUC: 0.7754 - val_loss: 0.5826 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7889 - loss: 0.5641 - val_AUC: 0.7790 - val_loss: 0.5789 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7931 - loss: 0.5601 - val_AUC: 0.7763 - val_loss: 0.5839 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7948 - loss: 0.5588 - val_AUC: 0.7803 - val_loss: 0.5815 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7981 - loss: 0.5552 - val_AUC: 0.7800 - val_loss: 0.5795 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7974 - loss: 0.5560 - val_AUC: 0.7818 - val_loss: 0.5766 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8009 - loss: 0.5522 - val_AUC: 0.7825 - val_loss: 0.5779 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8023 - loss: 0.5508 - val_AUC: 0.7807 - val_loss: 0.5803 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8026 - loss: 0.5504 - val_AUC: 0.7835 - val_loss: 0.5819 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8050 - loss: 0.5481 - val_AUC: 0.7818 - val_loss: 0.5838 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8084 - loss: 0.5445 - val_AUC: 0.7806 - val_loss: 0.5831 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8074 - loss: 0.5456 - val_AUC: 0.7801 - val_loss: 0.5857 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8116 - loss: 0.5404 - val_AUC: 0.7783 - val_loss: 0.5876 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8110 - loss: 0.5411 - val_AUC: 0.7856 - val_loss: 0.5820 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8147 - loss: 0.5374 - val_AUC: 0.7835 - val_loss: 0.5839 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8173 - loss: 0.5342 - val_AUC: 0.7819 - val_loss: 0.5875 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8173 - loss: 0.5336 - val_AUC: 0.7789 - val_loss: 0.5915 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8189 - loss: 0.5327 - val_AUC: 0.7822 - val_loss: 0.5831 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8216 - loss: 0.5299 - val_AUC: 0.7869 - val_loss: 0.5808 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8225 - loss: 0.5291 - val_AUC: 0.7822 - val_loss: 0.5856 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8220 - loss: 0.5287 - val_AUC: 0.7824 - val_loss: 0.5890 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8256 - loss: 0.5247 - val_AUC: 0.7856 - val_loss: 0.5812 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8271 - loss: 0.5231 - val_AUC: 0.7862 - val_loss: 0.5851 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m348/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8283 - loss: 0.5220\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8283 - loss: 0.5220 - val_AUC: 0.7865 - val_loss: 0.5825 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8341 - loss: 0.5147 - val_AUC: 0.7921 - val_loss: 0.5811 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8361 - loss: 0.5121 - val_AUC: 0.7899 - val_loss: 0.5838 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8392 - loss: 0.5075 - val_AUC: 0.7909 - val_loss: 0.5813 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8401 - loss: 0.5059 - val_AUC: 0.7900 - val_loss: 0.5836 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8446 - loss: 0.5004 - val_AUC: 0.7892 - val_loss: 0.5823 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m349/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8448 - loss: 0.5001\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8449 - loss: 0.5000 - val_AUC: 0.7905 - val_loss: 0.5846 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8474 - loss: 0.4970 - val_AUC: 0.7933 - val_loss: 0.5819 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8504 - loss: 0.4925 - val_AUC: 0.7928 - val_loss: 0.5800 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8507 - loss: 0.4922 - val_AUC: 0.7937 - val_loss: 0.5810 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8538 - loss: 0.4877 - val_AUC: 0.7925 - val_loss: 0.5840 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8522 - loss: 0.4902 - val_AUC: 0.7928 - val_loss: 0.5805 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8554 - loss: 0.4853 - val_AUC: 0.7931 - val_loss: 0.5839 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8519 - loss: 0.4905 - val_AUC: 0.7919 - val_loss: 0.5831 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8533 - loss: 0.4877\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8534 - loss: 0.4876 - val_AUC: 0.7933 - val_loss: 0.5832 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8561 - loss: 0.4841 - val_AUC: 0.7944 - val_loss: 0.5810 - learning_rate: 1.2500e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8574 - loss: 0.4823 - val_AUC: 0.7938 - val_loss: 0.5818 - learning_rate: 1.2500e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8574 - loss: 0.4823 - val_AUC: 0.7937 - val_loss: 0.5827 - learning_rate: 1.2500e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8574 - loss: 0.4820 - val_AUC: 0.7940 - val_loss: 0.5831 - learning_rate: 1.2500e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8584 - loss: 0.4809 - val_AUC: 0.7935 - val_loss: 0.5841 - learning_rate: 1.2500e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m348/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8606 - loss: 0.4777\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8606 - loss: 0.4776 - val_AUC: 0.7929 - val_loss: 0.5844 - learning_rate: 1.2500e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8570 - loss: 0.4827 - val_AUC: 0.7937 - val_loss: 0.5818 - learning_rate: 6.2500e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8621 - loss: 0.4755 - val_AUC: 0.7937 - val_loss: 0.5828 - learning_rate: 6.2500e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8603 - loss: 0.4778 - val_AUC: 0.7938 - val_loss: 0.5833 - learning_rate: 6.2500e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8630 - loss: 0.4741 - val_AUC: 0.7935 - val_loss: 0.5830 - learning_rate: 6.2500e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m343/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8624 - loss: 0.4747\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8625 - loss: 0.4745 - val_AUC: 0.7928 - val_loss: 0.5850 - learning_rate: 6.2500e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Model 20/20 | HP Config 5 | Seed 300\n",
      "Epoch 1/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - AUC: 0.5945 - loss: 0.7229 - val_AUC: 0.6666 - val_loss: 0.6860 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6626 - loss: 0.6638 - val_AUC: 0.6914 - val_loss: 0.6480 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6877 - loss: 0.6467 - val_AUC: 0.7069 - val_loss: 0.6366 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7082 - loss: 0.6331 - val_AUC: 0.7183 - val_loss: 0.6307 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7268 - loss: 0.6200 - val_AUC: 0.7277 - val_loss: 0.6243 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7477 - loss: 0.6034 - val_AUC: 0.7506 - val_loss: 0.6018 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7556 - loss: 0.5969 - val_AUC: 0.7512 - val_loss: 0.6010 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7645 - loss: 0.5888 - val_AUC: 0.7584 - val_loss: 0.5952 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7694 - loss: 0.5842 - val_AUC: 0.7607 - val_loss: 0.5926 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7724 - loss: 0.5816 - val_AUC: 0.7646 - val_loss: 0.5891 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7797 - loss: 0.5746 - val_AUC: 0.7645 - val_loss: 0.5898 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7789 - loss: 0.5752 - val_AUC: 0.7687 - val_loss: 0.5840 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7825 - loss: 0.5717 - val_AUC: 0.7720 - val_loss: 0.5820 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7872 - loss: 0.5668 - val_AUC: 0.7728 - val_loss: 0.5828 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7897 - loss: 0.5641 - val_AUC: 0.7706 - val_loss: 0.5863 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7922 - loss: 0.5615 - val_AUC: 0.7767 - val_loss: 0.5778 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7922 - loss: 0.5613 - val_AUC: 0.7770 - val_loss: 0.5778 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7964 - loss: 0.5573 - val_AUC: 0.7800 - val_loss: 0.5770 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7998 - loss: 0.5536 - val_AUC: 0.7850 - val_loss: 0.5695 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8010 - loss: 0.5525 - val_AUC: 0.7831 - val_loss: 0.5745 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8017 - loss: 0.5519 - val_AUC: 0.7839 - val_loss: 0.5717 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8059 - loss: 0.5472 - val_AUC: 0.7817 - val_loss: 0.5755 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8070 - loss: 0.5453 - val_AUC: 0.7826 - val_loss: 0.5757 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m343/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8089 - loss: 0.5434\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8089 - loss: 0.5434 - val_AUC: 0.7823 - val_loss: 0.5793 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8147 - loss: 0.5368 - val_AUC: 0.7939 - val_loss: 0.5619 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8190 - loss: 0.5317 - val_AUC: 0.7928 - val_loss: 0.5632 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8214 - loss: 0.5287 - val_AUC: 0.7921 - val_loss: 0.5664 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8210 - loss: 0.5292 - val_AUC: 0.7931 - val_loss: 0.5665 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8238 - loss: 0.5260 - val_AUC: 0.7930 - val_loss: 0.5660 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m345/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8275 - loss: 0.5209\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8275 - loss: 0.5209 - val_AUC: 0.7936 - val_loss: 0.5661 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8298 - loss: 0.5183 - val_AUC: 0.7970 - val_loss: 0.5594 - learning_rate: 2.5000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8301 - loss: 0.5175 - val_AUC: 0.7973 - val_loss: 0.5590 - learning_rate: 2.5000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8296 - loss: 0.5182 - val_AUC: 0.7958 - val_loss: 0.5603 - learning_rate: 2.5000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8308 - loss: 0.5162 - val_AUC: 0.7966 - val_loss: 0.5605 - learning_rate: 2.5000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8328 - loss: 0.5140 - val_AUC: 0.7962 - val_loss: 0.5619 - learning_rate: 2.5000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8342 - loss: 0.5127 - val_AUC: 0.7961 - val_loss: 0.5615 - learning_rate: 2.5000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8314 - loss: 0.5161 - val_AUC: 0.7975 - val_loss: 0.5605 - learning_rate: 2.5000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8356 - loss: 0.5105 - val_AUC: 0.7955 - val_loss: 0.5627 - learning_rate: 2.5000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8354 - loss: 0.5111 - val_AUC: 0.7974 - val_loss: 0.5598 - learning_rate: 2.5000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8372 - loss: 0.5088 - val_AUC: 0.7959 - val_loss: 0.5628 - learning_rate: 2.5000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8360 - loss: 0.5098 - val_AUC: 0.7964 - val_loss: 0.5618 - learning_rate: 2.5000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m343/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8373 - loss: 0.5074\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8373 - loss: 0.5074 - val_AUC: 0.7965 - val_loss: 0.5610 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8402 - loss: 0.5040 - val_AUC: 0.7984 - val_loss: 0.5595 - learning_rate: 1.2500e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8390 - loss: 0.5058 - val_AUC: 0.7986 - val_loss: 0.5591 - learning_rate: 1.2500e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8394 - loss: 0.5051 - val_AUC: 0.7983 - val_loss: 0.5594 - learning_rate: 1.2500e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8417 - loss: 0.5022 - val_AUC: 0.7980 - val_loss: 0.5610 - learning_rate: 1.2500e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8410 - loss: 0.5032 - val_AUC: 0.7978 - val_loss: 0.5603 - learning_rate: 1.2500e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8425 - loss: 0.5009 - val_AUC: 0.7982 - val_loss: 0.5608 - learning_rate: 1.2500e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m344/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8419 - loss: 0.5014\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8419 - loss: 0.5013 - val_AUC: 0.7980 - val_loss: 0.5610 - learning_rate: 1.2500e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8444 - loss: 0.4981 - val_AUC: 0.7988 - val_loss: 0.5590 - learning_rate: 6.2500e-05\n",
      "Epoch 51/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8437 - loss: 0.4991 - val_AUC: 0.7990 - val_loss: 0.5594 - learning_rate: 6.2500e-05\n",
      "Epoch 52/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8445 - loss: 0.4976 - val_AUC: 0.7989 - val_loss: 0.5591 - learning_rate: 6.2500e-05\n",
      "Epoch 53/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8452 - loss: 0.4971 - val_AUC: 0.7983 - val_loss: 0.5603 - learning_rate: 6.2500e-05\n",
      "Epoch 54/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8448 - loss: 0.4979 - val_AUC: 0.7985 - val_loss: 0.5600 - learning_rate: 6.2500e-05\n",
      "Epoch 55/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8463 - loss: 0.4955 - val_AUC: 0.7986 - val_loss: 0.5600 - learning_rate: 6.2500e-05\n",
      "Epoch 56/200\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8448 - loss: 0.4974\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8448 - loss: 0.4974 - val_AUC: 0.7983 - val_loss: 0.5605 - learning_rate: 6.2500e-05\n",
      "Epoch 57/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8459 - loss: 0.4958 - val_AUC: 0.7987 - val_loss: 0.5602 - learning_rate: 3.1250e-05\n",
      "Epoch 58/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8457 - loss: 0.4963 - val_AUC: 0.7992 - val_loss: 0.5597 - learning_rate: 3.1250e-05\n",
      "Epoch 59/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8470 - loss: 0.4944 - val_AUC: 0.7989 - val_loss: 0.5602 - learning_rate: 3.1250e-05\n",
      "Epoch 60/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8459 - loss: 0.4958 - val_AUC: 0.7992 - val_loss: 0.5592 - learning_rate: 3.1250e-05\n",
      "Epoch 61/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8448 - loss: 0.4978 - val_AUC: 0.7991 - val_loss: 0.5592 - learning_rate: 3.1250e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8470 - loss: 0.4946 - val_AUC: 0.7992 - val_loss: 0.5594 - learning_rate: 3.1250e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8467 - loss: 0.4957\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8467 - loss: 0.4957 - val_AUC: 0.7990 - val_loss: 0.5602 - learning_rate: 3.1250e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8457 - loss: 0.4964 - val_AUC: 0.7992 - val_loss: 0.5597 - learning_rate: 1.5625e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8448 - loss: 0.4981 - val_AUC: 0.7993 - val_loss: 0.5595 - learning_rate: 1.5625e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8483 - loss: 0.4933 - val_AUC: 0.7994 - val_loss: 0.5598 - learning_rate: 1.5625e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8468 - loss: 0.4945 - val_AUC: 0.7994 - val_loss: 0.5595 - learning_rate: 1.5625e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8433 - loss: 0.4995 - val_AUC: 0.7994 - val_loss: 0.5594 - learning_rate: 1.5625e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8465 - loss: 0.4953 - val_AUC: 0.7994 - val_loss: 0.5596 - learning_rate: 1.5625e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m349/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8463 - loss: 0.4951\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8463 - loss: 0.4951 - val_AUC: 0.7991 - val_loss: 0.5595 - learning_rate: 1.5625e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8484 - loss: 0.4925 - val_AUC: 0.7992 - val_loss: 0.5596 - learning_rate: 7.8125e-06\n",
      "Epoch 72/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8492 - loss: 0.4911 - val_AUC: 0.7990 - val_loss: 0.5597 - learning_rate: 7.8125e-06\n",
      "Epoch 73/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8451 - loss: 0.4967 - val_AUC: 0.7993 - val_loss: 0.5595 - learning_rate: 7.8125e-06\n",
      "Epoch 74/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8497 - loss: 0.4902 - val_AUC: 0.7992 - val_loss: 0.5596 - learning_rate: 7.8125e-06\n",
      "Epoch 75/200\n",
      "\u001b[1m346/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.8476 - loss: 0.4938\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8476 - loss: 0.4938 - val_AUC: 0.7992 - val_loss: 0.5598 - learning_rate: 7.8125e-06\n",
      "Epoch 76/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8458 - loss: 0.4960 - val_AUC: 0.7992 - val_loss: 0.5596 - learning_rate: 3.9063e-06\n",
      "Epoch 77/200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.8472 - loss: 0.4939 - val_AUC: 0.7992 - val_loss: 0.5596 - learning_rate: 3.9063e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All models trained and saved for ensembling!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Train Multiple Models with the Top 5 Best Hyperparameters and Different Seeds\n",
    "num_hyperparams = 5  # Number of top hyperparameter configurations\n",
    "models_per_hyperparam = 4  # Number of models per hyperparameter set\n",
    "total_models = num_hyperparams * models_per_hyperparam  # Total models to train\n",
    "\n",
    "#  Retrieve the top 5 best hyperparameters\n",
    "best_hyperparams_list = tuner.get_best_hyperparameters(num_trials=num_hyperparams)\n",
    "\n",
    "model_paths = []\n",
    "model_index = 1  # Model naming index\n",
    "\n",
    "for hp_index, best_hps in enumerate(best_hyperparams_list):\n",
    "    for seed_index in range(models_per_hyperparam):\n",
    "        print(f\"🔹 Training Model {model_index}/{total_models} | HP Config {hp_index+1} | Seed {seed_index*100}\")\n",
    "\n",
    "        #  Set different random seed per model\n",
    "        tf.random.set_seed(seed_index * 100)\n",
    "        np.random.seed(seed_index * 100)\n",
    "\n",
    "        #  Build and Train Model with the Selected Hyperparameters\n",
    "        model = tuner.hypermodel.build(best_hps)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=200,  # Train longer\n",
    "            batch_size = best_hps.values.get(\"batch_size\", 128),\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping, lr_scheduler]\n",
    "        )\n",
    "\n",
    "        #  Save Model\n",
    "        model_path = f\"best_model_{model_index}.h5\"\n",
    "        model.save(model_path)\n",
    "        model_paths.append(model_path)\n",
    "\n",
    "        model_index += 1  # Increment model index\n",
    "\n",
    "print(\" All models trained and saved for ensembling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 415us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 424us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 413us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 419us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 587us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 542us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 557us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 993us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 993us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 975us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 664us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 670us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 494us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 495us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 492us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 497us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 937us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 932us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 651us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649us/step\n",
      "✅ Final Ensemble AUC on training set: 0.8623\n",
      "✅ Final Ensemble AUC on validation set: 0.8083\n",
      "✅ sample_submission_higgs_ensemble.csv saved!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Ensemble Predictions\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "#  Load Saved Models\n",
    "models = [tf.keras.models.load_model(path) for path in model_paths]\n",
    "\n",
    "#  Generate Predictions from All Models\n",
    "train_preds = np.array([model.predict(X_scaled)[:, 0] for model in models])\n",
    "val_preds = np.array([model.predict(X_val)[:, 0] for model in models])\n",
    "test_preds = np.array([model.predict(test_scaled)[:, 0] for model in models])\n",
    "\n",
    "# Compute validation AUC for each model\n",
    "val_aucs = [roc_auc_score(y_val, val_preds[i]) for i in range(len(models))]\n",
    "\n",
    "# Normalize AUCs to sum to 1 (turn them into weights)\n",
    "weights = np.array(val_aucs) / np.sum(val_aucs)\n",
    "\n",
    "# Apply weighted averaging for final predictions (weighted test averaging added)\n",
    "final_train_probs = np.average(train_preds, axis=0, weights=weights)\n",
    "final_val_probs = np.average(val_preds, axis=0, weights=weights)  # NEW: Weighted validation averaging\n",
    "final_test_probs = np.average(test_preds, axis=0, weights=weights)  # NEW: Weighted test averaging\n",
    "\n",
    "#  Compute AUC on Training Data\n",
    "train_fpr, train_tpr, _ = roc_curve(y, final_train_probs, pos_label=1)\n",
    "train_auc = auc(train_fpr, train_tpr)\n",
    "print(f\" Final Ensemble AUC on training set: {train_auc:.4f}\")\n",
    "\n",
    "#  Compute AUC on Validation Data (NEW)\n",
    "val_auc = roc_auc_score(y_val, final_val_probs)\n",
    "print(f\" Final Ensemble AUC on validation set: {val_auc:.4f}\")\n",
    "\n",
    "#  Predict on Test Set\n",
    "binary_preds = (final_test_probs >= 0.5).astype(int)\n",
    "\n",
    "#  Save Submission\n",
    "submission = np.column_stack((test.index.values, binary_preds))\n",
    "np.savetxt(\n",
    "    'sample_submission_higgs_ensemble.csv',\n",
    "    submission,\n",
    "    header='Id,Predicted',\n",
    "    delimiter=',',\n",
    "    comments='',\n",
    "    fmt=['%1.18e', '%1.18e']\n",
    ")\n",
    "\n",
    "print(\" sample_submission_higgs_ensemble.csv saved!\")\n",
    "\n",
    "#  Plot ROC Curve for Ensemble Validation\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_fpr, train_tpr, label=f'Ensemble Train ROC (AUC={train_auc:.3f})', color='blue')\n",
    "val_fpr, val_tpr, _ = roc_curve(y_val, final_val_probs)\n",
    "plt.plot(val_fpr, val_tpr, label=f'Ensemble Val ROC (AUC={val_auc:.3f})', color='red')\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Chance')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Ensemble Model - Training & Validation ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
